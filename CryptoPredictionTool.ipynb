{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statements\n",
    "---\n",
    "**Important note:**\n",
    "For some reason tensorflow version and numpy version have dependency conflicts. Need to figure out what version is stable for both of these to work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd                 # Pandas dataframe library\n",
    "import pandas_datareader as pdr     # Pandas datareader that allows me to lookup & store live crypto prices from yahoo finance.\n",
    "import numpy as np                  # Numpy\n",
    "import matplotlib.pyplot as pypl    # Pyplot used to create visuals/graphics based on data \n",
    "from alpha_vantage.timeseries import TimeSeries     # Library used for pulling live price data from alphavantage api\n",
    "\n",
    "from datetime import datetime, timedelta, timezone             # Datetime library.\n",
    "import pytz\n",
    "import json\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=ResourceWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import glob                         # For changing/finding proper directory\n",
    "import os                           # For changing/finding proper directory (when opening files)\n",
    "import requests\n",
    "import twint                        # Twitter web scraping tool with more features than the regular twitter API\n",
    "import nest_asyncio                 # Import required for twint usage.\n",
    "nest_asyncio.apply()                \n",
    "\n",
    "import re                           # Regex for string cleaning (used for Textblob Sentiment Analysis)\n",
    "from textblob import TextBlob       # Textblob used for sentiment analysis of cleaned data.\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer    # Sentiment analysis tool that works great on determining social media sentiment.\n",
    "from newsapi import NewsApiClient   # NewsApiClient lets me look up/pull news articles relating to specified topics.\n",
    "import requests                     # Used for sending get requests to the NewsAPI client.\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler                          # Scaler used for scaling data (LSTMRNN Implementation)\n",
    "from sklearn.metrics import accuracy_score, classification_report       \n",
    "from sklearn.model_selection import train_test_split                    # Used for splitting data\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis    # Used for implementing SVM\n",
    "import tensorflow as tf                                                 # TF used for LSTMRNN Implmentation\n",
    "from keras.layers import Dense, Dropout, LSTM                           # Dense, dropout & lstm used for creating LSTMRNN \n",
    "from keras.models import Sequential                                     # Important because we're working with Sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in crypto price dataset\n",
    "---\n",
    "Section below reads csv files into pandas dataframes for interacting with. Also compiles list of coin names for twitter searching.\n",
    "\n",
    "### What to do next:\n",
    "* Retrieve Token labels from CSV file for searching by Cashtag on twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'c:\\Users\\Brand\\OneDrive\\Documents\\GitHub\\CryptoPredictionTool\\prices\\DailyPrices'\n",
    "extension = 'csv'\n",
    "os.chdir(path)\n",
    "daily_csv_files = glob.glob('*.{}'.format(extension))\n",
    "\n",
    "\n",
    "path = r'c:\\Users\\Brand\\OneDrive\\Documents\\GitHub\\CryptoPredictionTool\\prices\\HourlyPrices'\n",
    "os.chdir(path)\n",
    "hourly_csv_files = glob.glob('*.{}'.format(extension))\n",
    "\n",
    "# Compile list of all coin names for searching on twitter later\n",
    "daily_coins = []\n",
    "hourly_coins = []\n",
    "\n",
    "for coin in daily_csv_files:\n",
    "    vals = coin.split(\"_\")\n",
    "    coin_name = vals[1][:-4]\n",
    "    daily_coins.append(coin_name)\n",
    "\n",
    "for coin in hourly_csv_files:\n",
    "    vals = coin.split(\"_\")\n",
    "    coin_name = vals[0]\n",
    "    hourly_coins.append(coin_name)\n",
    "\n",
    "# compile list of pandas dataframes for use later.\n",
    "hourly_coin_data = []\n",
    "\n",
    "for file in hourly_csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    hourly_coin_data.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           unix                 date    symbol    open    high     low  \\\n",
      "0    1649980800  2022-04-15 00:00:00  AAVE/USD  173.36  174.49  173.36   \n",
      "1    1649977200  2022-04-14 23:00:00  AAVE/USD  172.55  172.55  172.55   \n",
      "2    1649973600  2022-04-14 22:00:00  AAVE/USD  173.50  173.50  173.50   \n",
      "3    1649970000  2022-04-14 21:00:00  AAVE/USD  172.14  173.01  172.14   \n",
      "4    1649966400  2022-04-14 20:00:00  AAVE/USD  167.79  169.82  167.22   \n",
      "..          ...                  ...       ...     ...     ...     ...   \n",
      "311  1648861200  2022-04-02 01:00:00  AAVE/USD  247.31  247.31  245.64   \n",
      "312  1648857600  2022-04-02 00:00:00  AAVE/USD  242.29  256.74  242.29   \n",
      "313  1648854000  2022-04-01 23:00:00  AAVE/USD  246.88  246.88  244.55   \n",
      "314  1648850400  2022-04-01 22:00:00  AAVE/USD  250.60  250.60  246.39   \n",
      "315  1648846800  2022-04-01 21:00:00  AAVE/USD  252.84  254.52  249.91   \n",
      "\n",
      "      close  Volume AAVE    Volume USD  \n",
      "0    174.49     0.116521     20.331669  \n",
      "1    172.55     7.221902   1246.139225  \n",
      "2    173.50     0.120000     20.820000  \n",
      "3    173.01     5.058670    875.200455  \n",
      "4    169.82   102.891824  17473.089548  \n",
      "..      ...          ...           ...  \n",
      "311  246.04    37.644060   9261.944540  \n",
      "312  251.79    55.034548  13857.148899  \n",
      "313  246.07    36.815549   9059.202069  \n",
      "314  247.57    19.513300   4830.907785  \n",
      "315  250.53    86.272691  21613.897361  \n",
      "\n",
      "[316 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(hourly_coin_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE:* The cell below is for reading in the Bitcoin tweets dataset from Kaggle. (https://www.kaggle.com/datasets/kaushiksuresh147/bitcoin-tweets)\n",
    "This datset kinda sucks though. For a few reasons:\n",
    "* Firstly, its tweets span 1.5 years but are only from 43 total days, making it inconsistent to use with Sequential data, like the price history.\n",
    "* Secondly, it has some values in impropere columns (namely tag values in the date column) which have to be manually removed.\n",
    "* Lastly, its huge. 280k tweets. Which at first seems great, but being that the sample size itself is incredibly sparse in terms of date-span, this leads to problems with implementation. \n",
    "\n",
    "I'll leave it here in a cell in case I decide to use it later, but for now, it doesn't apply to this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!! BELOW IS THE LOGIC FOR READING IN THE TWEETS FROM THE BITCOIN TWEET KAGGLE DATASET !!!\n",
    "# Note: This dataset kinda sucks. It has some values in the \n",
    "\n",
    "# Logic for reading in Bitcoin tweets dataset.\n",
    "# btc_tweets = pd.read_csv('../bitcoin_tweets/Bitcoin_tweets.csv')\n",
    "# btc_tweets.drop([64943], axis=0, inplace=True)\n",
    "# btc_tweets.drop([137068], axis=0, inplace=True)\n",
    "# btc_tweets.drop([180575], axis=0, inplace=True)\n",
    "\n",
    "# btc_tweets.drop(btc_tweets.index[100000:len(btc_tweets)], inplace=True)\n",
    "# btc_tweets.drop(columns=['user_name', 'user_location', 'user_description', 'user_created', 'user_followers', 'user_friends', 'user_favourites', 'user_verified', 'source', 'is_retweet'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\WaKaBurd\\\\Documents\\\\GitHub\\\\CryptoPredictionTool\\\\search_results'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Brand\\OneDrive\\Documents\\GitHub\\CryptoPredictionTool\\CryptoPredictionTool.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Brand/OneDrive/Documents/GitHub/CryptoPredictionTool/CryptoPredictionTool.ipynb#ch0000007?line=0'>1</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mc:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mUsers\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mWaKaBurd\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mDocuments\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mGitHub\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mCryptoPredictionTool\u001b[39m\u001b[39m\\\u001b[39m\u001b[39msearch_results\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Brand/OneDrive/Documents/GitHub/CryptoPredictionTool/CryptoPredictionTool.ipynb#ch0000007?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m coin \u001b[39min\u001b[39;00m hourly_coins: \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Brand/OneDrive/Documents/GitHub/CryptoPredictionTool/CryptoPredictionTool.ipynb#ch0000007?line=2'>3</a>\u001b[0m         os\u001b[39m.\u001b[39;49mchdir(path)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Brand/OneDrive/Documents/GitHub/CryptoPredictionTool/CryptoPredictionTool.ipynb#ch0000007?line=3'>4</a>\u001b[0m         os\u001b[39m.\u001b[39mmkdir(coin)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\WaKaBurd\\\\Documents\\\\GitHub\\\\CryptoPredictionTool\\\\search_results'"
     ]
    }
   ],
   "source": [
    "path = r'c:\\Users\\WaKaBurd\\Documents\\GitHub\\CryptoPredictionTool\\search_results'\n",
    "for coin in hourly_coins: \n",
    "        os.chdir(path)\n",
    "        os.mkdir(coin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Twitter for data on all coins supplied by dataset\n",
    "---\n",
    "Below section of code searches through twitter using keywords. Uses sift_tweet() function to remove all unnecessary characters, links, emojis & words from tweets. Also uses Textblob to append polarity column to pandas df for tracking sentiment of tweets.\n",
    "\n",
    "### What to do next:\n",
    "* Search twitter based on Cashtags & Hashtags\n",
    "* Configure Twint with Google translater so I can translate tweets from non-english langauges to english. (Need to create ticket for this in Github)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing twitter search for coin: AAVE\n",
      "searching 2022-04-17 to 2022-04-19\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "performing twitter search for coin: AVAX\n",
      "searching 2022-04-17 to 2022-04-19\n",
      "performing twitter search for coin: BCH\n",
      "searching 2022-04-17 to 2022-04-19\n",
      "performing twitter search for coin: BTC\n",
      "searching 2022-04-17 to 2022-04-19\n",
      "performing twitter search for coin: ETH\n",
      "searching 2022-04-17 to 2022-04-19\n"
     ]
    }
   ],
   "source": [
    "# Function for iterating through coins list and storing findings in .csv files\n",
    "def search_coins(coins):\n",
    "    important_cols = ['date', 'created_at', 'tweet']\n",
    "    coin_counter = 0\n",
    "    \n",
    "    for coin in coins:\n",
    "        path = r'c:\\Users\\Brand\\OneDrive\\Documents\\GitHub\\CryptoPredictionTool\\search_results'\n",
    "        os.chdir(path)\n",
    "        #os.mkdir(coin)\n",
    "        os.chdir(coin)\n",
    "        \n",
    "        print('performing twitter search for coin:', coin)\n",
    "        \n",
    "        from_date = '2022-04-17'\n",
    "        to_date = '2022-04-19'\n",
    "        #coin = \"Bitcoin\"\n",
    "        print(f'searching {from_date} to {to_date}')\n",
    "        \n",
    "        c = twint.Config()\n",
    "        c.Limit = 3000\n",
    "        c.Lang = \"en\"\n",
    "        c.Pandas = True\n",
    "        c.Search = coin\n",
    "        c.Hide_output = True\n",
    "        c.Since = from_date\n",
    "        c.Until = to_date\n",
    "        c.Store_csv = True\n",
    "        c.Output = coin + '_' + from_date + '_' + to_date + '_search_result.csv'\n",
    "        twint.run.Search(c)\n",
    "\n",
    "\n",
    "# btc_tweets.text=btc_tweets.text.astype(str)\n",
    "# btc_tweets['Processed Tweet'] = btc_tweets['text'].apply(lambda x: sift_tweet(x.lower(), stopwords)) \n",
    "# btc_tweets['Polarity/Subjectivity'] = btc_tweets['Processed Tweet'].apply(lambda x: TextBlob(x).sentiment)            \n",
    "\n",
    "# btc_tweets\n",
    "\n",
    "search_coins(hourly_coins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below chunk is more data pre-processing. \n",
    "I need to modify the dataframe so that it contains both the price information, as well as all of the tweets so I can easily perform sentiment analysis on them using VADER.\n",
    "\n",
    "The code below will read all CSV files that were stored in both the hourly_prices directory (done earlier) as well as the tweets that are searchd for and stored in the search results folders for each currency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lookin at coin number: 0\n",
      "lookin at coin number: 1\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "os.chdir(r'C:\\Users\\Brand\\OneDrive\\Documents\\GitHub\\CryptoPredictionTool\\archive')\n",
    "stopwords_file = open(\"stopwords.txt\", \"r+\")\n",
    "stopwords = list(stopwords_file.read().split('\\n'))\n",
    "\n",
    "# Need to create function for cleaning the tweets so we can derive the subjectivity and polarity using textblob.\n",
    "def sift_tweet(tweet, stop_words):\n",
    "    cleaned_tweet = tweet\n",
    "    cleaned_tweet = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",tweet) # regex to remove all @userame, emojis, and links from tweets.\n",
    "    for word in cleaned_tweet:\n",
    "        if word in stop_words: cleaned_tweet.replace(word, '')\n",
    "    return cleaned_tweet\n",
    "\n",
    "def get_sentiment(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment\n",
    "\n",
    "os.chdir(r'C:\\Users\\Brand\\OneDrive\\Documents\\GitHub\\CryptoPredictionTool\\search_results')\n",
    "tweet_pds = []\n",
    "grouped_tweets = []\n",
    "\n",
    "# Read Tweets into a DF from the CSVs\n",
    "for coin in hourly_coins:\n",
    "    \n",
    "    os.chdir(r'C:\\Users\\Brand\\OneDrive\\Documents\\GitHub\\CryptoPredictionTool\\search_results')\n",
    "    os.chdir(coin)\n",
    "    csv_names = glob.glob('*.{}'.format(extension))\n",
    "    coin_pds = []\n",
    "    for file in csv_names:\n",
    "        tweet_pd = pd.read_csv(file)\n",
    "        tweet_pd.sort_values(by='date')\n",
    "        coin_pds.append(tweet_pd)\n",
    "    tweet_pds.append(coin_pds)\n",
    "\n",
    "\n",
    "#for i in range(len(tweet_pds)):\n",
    "# This is just so i can the data i need to train a model for aave and avax. I'll do all 5 when i want to showcase something but for now i only need these 2.\n",
    "for i in range(2):\n",
    "    print('lookin at coin number:', i)\n",
    "    hourly_coin_data[i]['date'] = pd.to_datetime(hourly_coin_data[i]['date'])\n",
    "    hourly_coin_data[i]['joined_tweets'] = np.nan\n",
    "    hourly_coin_data[i]['compound'] = np.nan\n",
    "    hourly_coin_data[i]['positive'] = np.nan\n",
    "    hourly_coin_data[i]['negative'] = np.nan\n",
    "    hourly_coin_data[i]['neutral'] = np.nan\n",
    "\n",
    "    #print(hourly_coin_data[i])\n",
    "    for j in range(len(tweet_pds[i])):\n",
    "        tweet_pds[i][j]['date'] = pd.to_datetime(tweet_pds[i][j]['date'])\n",
    "\n",
    "        for day in range(1,31):\n",
    "            #print('checking day:', day)\n",
    "            for hour in range(24):\n",
    "                tweet_time_mask = (tweet_pds[i][j]['date'].dt.hour >= hour) & (tweet_pds[i][j]['date'].dt.hour < hour + 1) & \\\n",
    "                            (tweet_pds[i][j]['date'].dt.day >= day ) & (tweet_pds[i][j]['date'].dt.day < day + 1)\n",
    "                price_time_mask = (hourly_coin_data[i]['date'].dt.hour >= hour) & (hourly_coin_data[i]['date'].dt.hour < hour + 1) & \\\n",
    "                            (hourly_coin_data[i]['date'].dt.day >= day ) & (hourly_coin_data[i]['date'].dt.day < day + 1)\n",
    "\n",
    "                hour_view = tweet_pds[i][j][tweet_time_mask]\n",
    "                if hour_view.empty:\n",
    "                    continue\n",
    "                \n",
    "                hour_view['cleaned_tweet'] = hour_view['tweet'].apply(lambda x: sift_tweet(str(x).lower(), stopwords))\n",
    "\n",
    "                joined_tweets = ' '.join(hour_view['tweet'])\n",
    "                joined_clean_tweets = ' '.join(hour_view['cleaned_tweet'])\n",
    "\n",
    "                SIA = get_sentiment(joined_tweets)\n",
    "                compound = SIA['compound']                    # Score representing sum(lexicon ratings)\n",
    "                pos = SIA['pos']\n",
    "                neg = SIA['neg']\n",
    "                neu = SIA['neu']\n",
    "\n",
    "                index = hourly_coin_data[i][price_time_mask].index\n",
    "                for ind in index:\n",
    "                    hourly_coin_data[i].at[ind,'joined_tweets'] = joined_tweets\n",
    "                    hourly_coin_data[i].at[ind,'polarity'] = TextBlob(joined_clean_tweets).sentiment[0]            # Lambda function for creating Polarity value in Coin Dataframe using Textblob\n",
    "                    hourly_coin_data[i].at[ind,'subjectivity'] = TextBlob(joined_clean_tweets).sentiment[1]            # Lambda function for creating Polarity value in Coin Dataframe using Textblob\n",
    "                    hourly_coin_data[i].at[ind,'compound'] = compound\n",
    "                    hourly_coin_data[i].at[ind,'positive'] = pos\n",
    "                    hourly_coin_data[i].at[ind,'negative'] = neg\n",
    "                    hourly_coin_data[i].at[ind,'neutral'] = neu\n",
    "                \n",
    "                #Processing twitter live tweets\n",
    "\n",
    "\n",
    "                # hourly_coin_data[i]['Polarity'] = hourly_coin_data[i]['Processed Tweet'].apply(lambda x: TextBlob(x).sentiment[0])            # Lambda function for creating Polarity value in Coin Dataframe using Textblob\n",
    "                # hourly_coin_data[i]['Subjectivity'] = hourly_coin_data[i]['Processed Tweet'].apply(lambda x: TextBlob(x).sentiment[1])            # Lambda function for creating Polarity value in Coin Dataframe using Textblob\n",
    "\n",
    "                \n",
    "\n",
    "    hourly_coin_data[i] = hourly_coin_data[i][hourly_coin_data[i]['joined_tweets'].notna()]     # Drop all NAN rows (rows we don't have tweets for)\n",
    "\n",
    "print(len(tweet_pds[0]))\n",
    "#pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appending a label to each row signifying an increase/decrease in price during that hour. \n",
    "\n",
    "If the price increases/no change we append a 1 to the price change column for that row, if it decreases we append a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(hourly_coin_data)):\n",
    "    hourly_coin_data[i].reset_index()\n",
    "    hourly_coin_data[i]['price_change'] = np.nan\n",
    "    for index, row in hourly_coin_data[i].iterrows():\n",
    "        if row.open > row.close:\n",
    "            hourly_coin_data[i].at[index, 'price_change'] = 0\n",
    "        else:\n",
    "            hourly_coin_data[i].at[index, 'price_change'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_coin_data[3].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Train Neural Net on Dataset (Attempt 1)\n",
    "---\n",
    "\n",
    "\n",
    "### What to do next:\n",
    "* Probably attempt it differently. Outcomes are horrid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proper Implementation: LDA With Sentiment Analysis\n",
    "---\n",
    "Yeah the last one wasn't good. This one is ight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.56      0.67         9\n",
      "         1.0       0.50      0.80      0.62         5\n",
      "\n",
      "    accuracy                           0.64        14\n",
      "   macro avg       0.67      0.68      0.64        14\n",
      "weighted avg       0.71      0.64      0.65        14\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         0\n",
      "         1.0       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.75         4\n",
      "   macro avg       0.50      0.38      0.43         4\n",
      "weighted avg       1.00      0.75      0.86         4\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         8\n",
      "         1.0       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        11\n",
      "   macro avg       1.00      1.00      1.00        11\n",
      "weighted avg       1.00      1.00      1.00        11\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      1.00      0.67         1\n",
      "         1.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# These are all the columns we actually want to keep for the purposes of training & using the model.\n",
    "model_cols = ['open', 'high', 'low', 'Volume USD', 'compound', 'positive', 'negative', 'neutral', 'polarity', 'subjectivity', 'price_change']\n",
    "os.chdir(r'C:\\Users\\Brand\\OneDrive\\Documents\\GitHub\\CryptoPredictionTool\\hourly_coin_data')\n",
    "\n",
    "for i in range(len(hourly_coin_data)):\n",
    "\n",
    "    model_df = hourly_coin_data[i][model_cols]\n",
    "    model_df.to_csv(f'model_df_{i}.csv')\n",
    "\n",
    "    # Feature Dataset\n",
    "    x = model_df\n",
    "    x.drop(['price_change'], axis=1)\n",
    "    np.asarray(x)\n",
    "\n",
    "    # Target Dataset\n",
    "    y = np.array(model_df['price_change'])\n",
    "\n",
    "    # split into test & train\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    # Create svm model\n",
    "    model = LinearDiscriminantAnalysis().fit(x_train, y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull data from the last hour to make prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request(url, headers, params, next_token=None):\n",
    "    params['next_token'] = next_token\n",
    "    response = requests.request('GET', url, headers=headers, params=params)\n",
    "    print('Endpoint response code:' + str(response.status_code))\n",
    "    if (response.status_code != 200):\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "def predict_trend(coin):\n",
    "\n",
    "    # Pull tweets from the last hour\n",
    "    path = r'c:\\Users\\Brand\\OneDrive\\Documents\\GitHub\\CryptoPredictionTool\\predicted_trends'\n",
    "    os.chdir(path)\n",
    "    #os.chdir(coin)\n",
    "\n",
    "    print('performing twitter search for coin:', coin)\n",
    "\n",
    "    # 1 hour ago\n",
    "    from_date = datetime.now(timezone.utc) - timedelta(hours = 1)\n",
    "    to_date = datetime.now(timezone.utc) + timedelta(seconds=-30)\n",
    "    \n",
    "    iso_from_date = from_date.isoformat()\n",
    "    iso_to_date = to_date.isoformat()\n",
    "\n",
    "    from_date = from_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    to_date = to_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    print(f'searching {from_date} to {to_date}')\n",
    "    \n",
    "    bearer_token = 'AAAAAAAAAAAAAAAAAAAAAJwBbgEAAAAAyi3tWb4jDN72EZqz6dcWgOIizuc%3DsC3xrWGrxPCwiKwqy2fINUgJDs2qKaZNlITIIy75Pss1oiMeTN'\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer {}\".format(bearer_token)\n",
    "    }\n",
    "\n",
    "    url = 'https://api.twitter.com/2/tweets/search/recent'\n",
    "\n",
    "    params = {\n",
    "        'query': coin,\n",
    "        'start_time': iso_from_date,\n",
    "        'end_time': iso_to_date,\n",
    "        'max_results': 10,\n",
    "        'next_token':{}\n",
    "    }\n",
    "\n",
    "    json_response = send_request(url, headers, params)\n",
    "    print(json.dumps(json_response, indent=4, sort_keys=True))\n",
    "    return json_response\n",
    "    # Run Textblob on tweets\n",
    "\n",
    "    # Analyze sentiment of tweets\n",
    "\n",
    "    # Pull financial data from the last hour\n",
    "\n",
    "    # Merge datasets\n",
    "\n",
    "    # Feed them into model\n",
    "\n",
    "# Pull tweets on topic from last 30 minutes\n",
    "fetched_tweets = predict_trend('ETH lang:en')\n",
    "fetched_tweets_df = pd.DataFrame(fetched_tweets['data'])\n",
    "fetched_tweets_df.to_csv('recently_fetched_tweets.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull financial data from yahoo finance for the current hour\n",
    "# Have to use Alphavantage api\n",
    "\n",
    "av_api_key = 'GD982KLZ6PZ69GQ0'\n",
    "path = r'c:\\Users\\Brand\\OneDrive\\Documents\\GitHub\\CryptoPredictionTool\\prices\\LivePrices'\n",
    "os.chdir(path)\n",
    "\n",
    "\n",
    "def get_prices(coin):\n",
    "    url = f'https://www.alphavantage.co/query?function=CRYPTO_INTRADAY&symbol={coin}&market=USD&interval=1min&apikey={av_api_key}&datatype=csv'\n",
    "    req = requests.get(url)\n",
    "    data = req.content\n",
    "    csv_file = open(f'{coin}_prices.csv','wb')\n",
    "    csv_file.write(data)\n",
    "    csv_file.close()\n",
    "    return\n",
    "\n",
    "get_prices('AVAX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1. Information': 'Crypto Intraday (60min) Time Series',\n",
       " '2. Digital Currency Code': 'AVAX',\n",
       " '3. Digital Currency Name': 'Avalanche',\n",
       " '4. Market Code': 'USD',\n",
       " '5. Market Name': 'United States Dollar',\n",
       " '6. Last Refreshed': '2022-04-20 04:00:00',\n",
       " '7. Interval': '60min',\n",
       " '8. Output Size': 'Compact',\n",
       " '9. Time Zone': 'UTC'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_hourly_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run textblob on tweets for polarity & subjectivity\n",
    "fetched_tweets."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4c0bf06c6142ddc920bc4833060833a5c39c864bf9bfacfcb217d05e37f17a2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
