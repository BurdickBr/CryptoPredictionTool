{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statements\n",
    "---\n",
    "**Important note:**\n",
    "For some reason tensorflow version and numpy version have dependency conflicts. Need to figure out what version is stable for both of these to work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd                 # Pandas dataframe library\n",
    "import pandas_datareader as pdr     # Pandas datareader that allows me to lookup & store live crypto prices from yahoo finance.\n",
    "import numpy as np                  # Numpy\n",
    "import matplotlib.pyplot as pypl    # Pyplot used to create visuals/graphics based on data \n",
    "import datetime as dt               # Datetime library.\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=ResourceWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import glob                         # For changing/finding proper directory\n",
    "import os                           # For changing/finding proper directory (when opening files)\n",
    "\n",
    "import twint                        # Twitter web scraping tool with more features than the regular twitter API\n",
    "import nest_asyncio                 # Import required for twint usage.\n",
    "nest_asyncio.apply()                \n",
    "\n",
    "import re                           # Regex for string cleaning (used for Textblob Sentiment Analysis)\n",
    "from textblob import TextBlob       # Textblob used for sentiment analysis of cleaned data.\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer    # Sentiment analysis tool that works great on determining social media sentiment.\n",
    "from newsapi import NewsApiClient   # NewsApiClient lets me look up/pull news articles relating to specified topics.\n",
    "import requests                     # Used for sending get requests to the NewsAPI client.\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler                          # Scaler used for scaling data (LSTMRNN Implementation)\n",
    "from sklearn.metrics import accuracy_score, classification_report       \n",
    "from sklearn.model_selection import train_test_split                    # Used for splitting data\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis    # Used for implementing SVM\n",
    "import tensorflow as tf                                                 # TF used for LSTMRNN Implmentation\n",
    "from keras.layers import Dense, Dropout, LSTM                           # Dense, dropout & lstm used for creating LSTMRNN \n",
    "from keras.models import Sequential                                     # Important because we're working with Sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in crypto price dataset\n",
    "---\n",
    "Section below reads csv files into pandas dataframes for interacting with. Also compiles list of coin names for twitter searching.\n",
    "\n",
    "### What to do next:\n",
    "* Retrieve Token labels from CSV file for searching by Cashtag on twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'c:\\Users\\Brand\\OneDrive\\Documents\\GitHub\\CryptoPredictionTool\\prices\\DailyPrices'\n",
    "extension = 'csv'\n",
    "os.chdir(path)\n",
    "daily_csv_files = glob.glob('*.{}'.format(extension))\n",
    "\n",
    "\n",
    "path = r'c:\\Users\\Brand\\OneDrive\\Documents\\GitHub\\CryptoPredictionTool\\prices\\HourlyPrices'\n",
    "os.chdir(path)\n",
    "hourly_csv_files = glob.glob('*.{}'.format(extension))\n",
    "\n",
    "# Compile list of all coin names for searching on twitter later\n",
    "daily_coins = []\n",
    "hourly_coins = []\n",
    "\n",
    "for coin in daily_csv_files:\n",
    "    vals = coin.split(\"_\")\n",
    "    coin_name = vals[1][:-4]\n",
    "    daily_coins.append(coin_name)\n",
    "\n",
    "for coin in hourly_csv_files:\n",
    "    vals = coin.split(\"_\")\n",
    "    coin_name = vals[0]\n",
    "    hourly_coins.append(coin_name)\n",
    "\n",
    "# compile list of pandas dataframes for use later.\n",
    "hourly_coin_data = []\n",
    "\n",
    "for file in hourly_csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    hourly_coin_data.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           unix                 date    symbol    open    high     low  \\\n",
      "0    1649980800  2022-04-15 00:00:00  AAVE/USD  173.36  174.49  173.36   \n",
      "1    1649977200  2022-04-14 23:00:00  AAVE/USD  172.55  172.55  172.55   \n",
      "2    1649973600  2022-04-14 22:00:00  AAVE/USD  173.50  173.50  173.50   \n",
      "3    1649970000  2022-04-14 21:00:00  AAVE/USD  172.14  173.01  172.14   \n",
      "4    1649966400  2022-04-14 20:00:00  AAVE/USD  167.79  169.82  167.22   \n",
      "..          ...                  ...       ...     ...     ...     ...   \n",
      "311  1648861200  2022-04-02 01:00:00  AAVE/USD  247.31  247.31  245.64   \n",
      "312  1648857600  2022-04-02 00:00:00  AAVE/USD  242.29  256.74  242.29   \n",
      "313  1648854000  2022-04-01 23:00:00  AAVE/USD  246.88  246.88  244.55   \n",
      "314  1648850400  2022-04-01 22:00:00  AAVE/USD  250.60  250.60  246.39   \n",
      "315  1648846800  2022-04-01 21:00:00  AAVE/USD  252.84  254.52  249.91   \n",
      "\n",
      "      close  Volume AAVE    Volume USD  \n",
      "0    174.49     0.116521     20.331669  \n",
      "1    172.55     7.221902   1246.139225  \n",
      "2    173.50     0.120000     20.820000  \n",
      "3    173.01     5.058670    875.200455  \n",
      "4    169.82   102.891824  17473.089548  \n",
      "..      ...          ...           ...  \n",
      "311  246.04    37.644060   9261.944540  \n",
      "312  251.79    55.034548  13857.148899  \n",
      "313  246.07    36.815549   9059.202069  \n",
      "314  247.57    19.513300   4830.907785  \n",
      "315  250.53    86.272691  21613.897361  \n",
      "\n",
      "[316 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(hourly_coin_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE:* The cell below is for reading in the Bitcoin tweets dataset from Kaggle. (https://www.kaggle.com/datasets/kaushiksuresh147/bitcoin-tweets)\n",
    "This datset kinda sucks though. For a few reasons:\n",
    "* Firstly, its tweets span 1.5 years but are only from 43 total days, making it inconsistent to use with Sequential data, like the price history.\n",
    "* Secondly, it has some values in impropere columns (namely tag values in the date column) which have to be manually removed.\n",
    "* Lastly, its huge. 280k tweets. Which at first seems great, but being that the sample size itself is incredibly sparse in terms of date-span, this leads to problems with implementation. \n",
    "\n",
    "I'll leave it here in a cell in case I decide to use it later, but for now, it doesn't apply to this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!! BELOW IS THE LOGIC FOR READING IN THE TWEETS FROM THE BITCOIN TWEET KAGGLE DATASET !!!\n",
    "# Note: This dataset kinda sucks. It has some values in the \n",
    "\n",
    "# Logic for reading in Bitcoin tweets dataset.\n",
    "# btc_tweets = pd.read_csv('../bitcoin_tweets/Bitcoin_tweets.csv')\n",
    "# btc_tweets.drop([64943], axis=0, inplace=True)\n",
    "# btc_tweets.drop([137068], axis=0, inplace=True)\n",
    "# btc_tweets.drop([180575], axis=0, inplace=True)\n",
    "\n",
    "# btc_tweets.drop(btc_tweets.index[100000:len(btc_tweets)], inplace=True)\n",
    "# btc_tweets.drop(columns=['user_name', 'user_location', 'user_description', 'user_created', 'user_followers', 'user_friends', 'user_favourites', 'user_verified', 'source', 'is_retweet'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'c:\\Users\\WaKaBurd\\Documents\\GitHub\\CryptoPredictionTool\\search_results'\n",
    "for coin in hourly_coins: \n",
    "        os.chdir(path)\n",
    "        os.mkdir(coin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Twitter for data on all coins supplied by dataset\n",
    "---\n",
    "Below section of code searches through twitter using keywords. Uses sift_tweet() function to remove all unnecessary characters, links, emojis & words from tweets. Also uses Textblob to append polarity column to pandas df for tracking sentiment of tweets.\n",
    "\n",
    "### What to do next:\n",
    "* Search twitter based on Cashtags & Hashtags\n",
    "* Configure Twint with Google translater so I can translate tweets from non-english langauges to english. (Need to create ticket for this in Github)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing twitter search for coin: AAVE\n",
      "searching 2022-04-10 to 2022-04-12\n",
      "performing twitter search for coin: AVAX\n",
      "searching 2022-04-10 to 2022-04-12\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "performing twitter search for coin: BCH\n",
      "searching 2022-04-10 to 2022-04-12\n",
      "performing twitter search for coin: BTC\n",
      "searching 2022-04-10 to 2022-04-12\n",
      "performing twitter search for coin: ETH\n",
      "searching 2022-04-10 to 2022-04-12\n"
     ]
    }
   ],
   "source": [
    "# Function for iterating through coins list and storing findings in .csv files\n",
    "def search_coins(coins):\n",
    "    important_cols = ['date', 'created_at', 'tweet']\n",
    "    coin_counter = 0\n",
    "    \n",
    "    for coin in coins:\n",
    "        path = r'c:\\Users\\Brand\\OneDrive\\Documents\\GitHub\\CryptoPredictionTool\\search_results'\n",
    "        os.chdir(path)\n",
    "        #os.mkdir(coin)\n",
    "        os.chdir(coin)\n",
    "        \n",
    "        print('performing twitter search for coin:', coin)\n",
    "        \n",
    "        # Comprises list of hours from our hourly dataset so we can run twint searches on those hours only.\n",
    "        hours = []\n",
    "        for hour in hourly_coin_data[coin_counter]['date']:\n",
    "            hours.append(hour)\n",
    "        \n",
    "        for i in range(len(hours)-1):\n",
    "            \n",
    "            # to_date = hours[i]\n",
    "            # from_date = hours[i+1]\n",
    "           \n",
    "            from_date = '2022-04-10'\n",
    "            to_date = '2022-04-12'\n",
    "            #coin = \"Bitcoin\"\n",
    "            print(f'searching {from_date} to {to_date}')\n",
    "            \n",
    "            c = twint.Config()\n",
    "            c.Limit = 3000\n",
    "            c.Lang = \"en\"\n",
    "            c.Pandas = True\n",
    "            c.Search = coin\n",
    "            c.Hide_output = True\n",
    "            c.Since = from_date\n",
    "            c.Until = to_date\n",
    "            c.Store_csv = True\n",
    "            c.Output = coin + '_' + from_date + '_' + to_date + '_search_result.csv'\n",
    "            twint.run.Search(c)\n",
    "            coin_df = twint.storage.panda.Tweets_df\n",
    "            \n",
    "            \n",
    "            # important for when twint fails to find tweets based on currency.\n",
    "            # if coin_df.empty:\n",
    "            #     print('no results, moving on...')\n",
    "            #     break\n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "            # from_date = re.sub(' ','-',from_date)\n",
    "            # from_date = re.sub(':','-',from_date)\n",
    "            \n",
    "            \n",
    "            #coin_df.to_csv(from_date + '-' + to_date + '.csv')\n",
    "\n",
    "# btc_tweets.text=btc_tweets.text.astype(str)\n",
    "# btc_tweets['Processed Tweet'] = btc_tweets['text'].apply(lambda x: sift_tweet(x.lower(), stopwords)) \n",
    "# btc_tweets['Polarity/Subjectivity'] = btc_tweets['Processed Tweet'].apply(lambda x: TextBlob(x).sentiment)            \n",
    "\n",
    "# btc_tweets\n",
    "\n",
    "search_coins(hourly_coins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below chunk is more data pre-processing. \n",
    "I need to modify the dataframe so that it contains both the price information, as well as all of the tweets so I can easily perform sentiment analysis on them using VADER.\n",
    "\n",
    "The code below will read all CSV files that were stored in both the hourly_prices directory (done earlier) as well as the tweets that are searchd for and stored in the search results folders for each currency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lookin at coin number: 0\n",
      "lookin at coin number: 1\n",
      "lookin at coin number: 2\n",
      "lookin at coin number: 3\n",
      "lookin at coin number: 4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "os.chdir(r'C:\\Users\\Brand\\OneDrive\\Documents\\GitHub\\CryptoPredictionTool\\archive')\n",
    "stopwords_file = open(\"stopwords.txt\", \"r+\")\n",
    "stopwords = list(stopwords_file.read().split('\\n'))\n",
    "\n",
    "# Need to create function for cleaning the tweets so we can derive the subjectivity and polarity using textblob.\n",
    "def sift_tweet(tweet, stop_words):\n",
    "    cleaned_tweet = tweet\n",
    "    cleaned_tweet = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",tweet) # regex to remove all @userame, emojis, and links from tweets.\n",
    "    for word in cleaned_tweet:\n",
    "        if word in stop_words: cleaned_tweet.replace(word, '')\n",
    "    return cleaned_tweet\n",
    "\n",
    "def get_sentiment(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment\n",
    "\n",
    "os.chdir(r'C:\\Users\\Brand\\OneDrive\\Documents\\GitHub\\CryptoPredictionTool\\search_results')\n",
    "tweet_pds = []\n",
    "grouped_tweets = []\n",
    "\n",
    "# Read Tweets into a DF from the CSVs\n",
    "for coin in hourly_coins:\n",
    "    \n",
    "    os.chdir(r'C:\\Users\\Brand\\OneDrive\\Documents\\GitHub\\CryptoPredictionTool\\search_results')\n",
    "    os.chdir(coin)\n",
    "    csv_names = glob.glob('*.{}'.format(extension))\n",
    "    coin_pds = []\n",
    "    for file in csv_names:\n",
    "        tweet_pd = pd.read_csv(file)\n",
    "        tweet_pd.sort_values(by='date')\n",
    "        coin_pds.append(tweet_pd)\n",
    "    tweet_pds.append(coin_pds)\n",
    "\n",
    "\n",
    "for i in range(len(tweet_pds)):\n",
    "    print('lookin at coin number:', i)\n",
    "    hourly_coin_data[i]['date'] = pd.to_datetime(hourly_coin_data[i]['date'])\n",
    "    hourly_coin_data[i]['joined_tweets'] = np.nan\n",
    "    hourly_coin_data[i]['compound'] = np.nan\n",
    "    hourly_coin_data[i]['positive'] = np.nan\n",
    "    hourly_coin_data[i]['negative'] = np.nan\n",
    "    hourly_coin_data[i]['neutral'] = np.nan\n",
    "\n",
    "    #print(hourly_coin_data[i])\n",
    "    for j in range(len(tweet_pds[i])):\n",
    "        tweet_pds[i][j]['date'] = pd.to_datetime(tweet_pds[i][j]['date'])\n",
    "\n",
    "        for day in range(1,15):\n",
    "            #print('checking day:', day)\n",
    "            for hour in range(24):\n",
    "                tweet_time_mask = (tweet_pds[i][j]['date'].dt.hour >= hour) & (tweet_pds[i][j]['date'].dt.hour < hour + 1) & \\\n",
    "                            (tweet_pds[i][j]['date'].dt.day >= day ) & (tweet_pds[i][j]['date'].dt.day < day + 1)\n",
    "                price_time_mask = (hourly_coin_data[i]['date'].dt.hour >= hour) & (hourly_coin_data[i]['date'].dt.hour < hour + 1) & \\\n",
    "                            (hourly_coin_data[i]['date'].dt.day >= day ) & (hourly_coin_data[i]['date'].dt.day < day + 1)\n",
    "\n",
    "                hour_view = tweet_pds[i][j][tweet_time_mask]\n",
    "                if hour_view.empty:\n",
    "                    continue\n",
    "                \n",
    "                hour_view['cleaned_tweet'] = hour_view['tweet'].apply(lambda x: sift_tweet(str(x).lower(), stopwords))\n",
    "\n",
    "                joined_tweets = ' '.join(hour_view['tweet'])\n",
    "                joined_clean_tweets = ' '.join(hour_view['cleaned_tweet'])\n",
    "\n",
    "                SIA = get_sentiment(joined_tweets)\n",
    "                compound = SIA['compound']                    # Score representing sum(lexicon ratings)\n",
    "                pos = SIA['pos']\n",
    "                neg = SIA['neg']\n",
    "                neu = SIA['neu']\n",
    "\n",
    "                index = hourly_coin_data[i][price_time_mask].index\n",
    "                for ind in index:\n",
    "                    hourly_coin_data[i].at[ind,'joined_tweets'] = joined_tweets\n",
    "                    hourly_coin_data[i].at[ind,'polarity'] = TextBlob(joined_clean_tweets).sentiment[0]            # Lambda function for creating Polarity value in Coin Dataframe using Textblob\n",
    "                    hourly_coin_data[i].at[ind,'subjectivity'] = TextBlob(joined_clean_tweets).sentiment[1]            # Lambda function for creating Polarity value in Coin Dataframe using Textblob\n",
    "                    hourly_coin_data[i].at[ind,'compound'] = compound\n",
    "                    hourly_coin_data[i].at[ind,'positive'] = pos\n",
    "                    hourly_coin_data[i].at[ind,'negative'] = neg\n",
    "                    hourly_coin_data[i].at[ind,'neutral'] = neu\n",
    "                \n",
    "                #Processing twitter live tweets\n",
    "\n",
    "\n",
    "                # hourly_coin_data[i]['Polarity'] = hourly_coin_data[i]['Processed Tweet'].apply(lambda x: TextBlob(x).sentiment[0])            # Lambda function for creating Polarity value in Coin Dataframe using Textblob\n",
    "                # hourly_coin_data[i]['Subjectivity'] = hourly_coin_data[i]['Processed Tweet'].apply(lambda x: TextBlob(x).sentiment[1])            # Lambda function for creating Polarity value in Coin Dataframe using Textblob\n",
    "\n",
    "                \n",
    "\n",
    "    hourly_coin_data[i] = hourly_coin_data[i][hourly_coin_data[i]['joined_tweets'].notna()]\n",
    "\n",
    "print(len(tweet_pds[0]))\n",
    "#pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appending a label to each row signifying an increase/decrease in price during that hour. \n",
    "\n",
    "If the price increases/no change we append a 1 to the price change column for that row, if it decreases we append a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(hourly_coin_data)):\n",
    "    hourly_coin_data[i].reset_index()\n",
    "    hourly_coin_data[i]['price_change'] = np.nan\n",
    "    for index, row in hourly_coin_data[i].iterrows():\n",
    "        if row.open > row.close:\n",
    "            hourly_coin_data[i].at[index, 'price_change'] = 0\n",
    "        else:\n",
    "            hourly_coin_data[i].at[index, 'price_change'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unix</th>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>Volume AAVE</th>\n",
       "      <th>Volume USD</th>\n",
       "      <th>joined_tweets</th>\n",
       "      <th>compound</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>price_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1649779200</td>\n",
       "      <td>2022-04-12 16:00:00</td>\n",
       "      <td>AAVE/USD</td>\n",
       "      <td>166.58</td>\n",
       "      <td>168.31</td>\n",
       "      <td>165.97</td>\n",
       "      <td>165.97</td>\n",
       "      <td>92.205446</td>\n",
       "      <td>15303.337894</td>\n",
       "      <td>Can’t believe it took so long for people in th...</td>\n",
       "      <td>-0.9944</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.038651</td>\n",
       "      <td>0.465325</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1649775600</td>\n",
       "      <td>2022-04-12 15:00:00</td>\n",
       "      <td>AAVE/USD</td>\n",
       "      <td>166.00</td>\n",
       "      <td>168.02</td>\n",
       "      <td>166.00</td>\n",
       "      <td>167.64</td>\n",
       "      <td>80.111635</td>\n",
       "      <td>13429.914508</td>\n",
       "      <td>@FinesseEness Recommended study material for t...</td>\n",
       "      <td>-0.9940</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.038002</td>\n",
       "      <td>0.415383</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1649772000</td>\n",
       "      <td>2022-04-12 14:00:00</td>\n",
       "      <td>AAVE/USD</td>\n",
       "      <td>169.74</td>\n",
       "      <td>170.29</td>\n",
       "      <td>164.42</td>\n",
       "      <td>166.01</td>\n",
       "      <td>199.818111</td>\n",
       "      <td>33171.804532</td>\n",
       "      <td>@Uniswap @AaveAave @compoundfinance @MakerDAO ...</td>\n",
       "      <td>-0.9954</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.041573</td>\n",
       "      <td>0.419930</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1649768400</td>\n",
       "      <td>2022-04-12 13:00:00</td>\n",
       "      <td>AAVE/USD</td>\n",
       "      <td>170.59</td>\n",
       "      <td>171.04</td>\n",
       "      <td>168.92</td>\n",
       "      <td>170.31</td>\n",
       "      <td>147.760822</td>\n",
       "      <td>25165.145528</td>\n",
       "      <td>@ColinWier @ThatYumiMa At least the worst song...</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.050538</td>\n",
       "      <td>0.436137</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1649764800</td>\n",
       "      <td>2022-04-12 12:00:00</td>\n",
       "      <td>AAVE/USD</td>\n",
       "      <td>169.35</td>\n",
       "      <td>169.93</td>\n",
       "      <td>168.78</td>\n",
       "      <td>169.57</td>\n",
       "      <td>80.897624</td>\n",
       "      <td>13717.810078</td>\n",
       "      <td>@lilyjanesmarkle @ZFiendy @beautifulrobot @Ivo...</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.032106</td>\n",
       "      <td>0.428238</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          unix                date    symbol    open    high     low   close  \\\n",
       "56  1649779200 2022-04-12 16:00:00  AAVE/USD  166.58  168.31  165.97  165.97   \n",
       "57  1649775600 2022-04-12 15:00:00  AAVE/USD  166.00  168.02  166.00  167.64   \n",
       "58  1649772000 2022-04-12 14:00:00  AAVE/USD  169.74  170.29  164.42  166.01   \n",
       "59  1649768400 2022-04-12 13:00:00  AAVE/USD  170.59  171.04  168.92  170.31   \n",
       "60  1649764800 2022-04-12 12:00:00  AAVE/USD  169.35  169.93  168.78  169.57   \n",
       "\n",
       "    Volume AAVE    Volume USD  \\\n",
       "56    92.205446  15303.337894   \n",
       "57    80.111635  13429.914508   \n",
       "58   199.818111  33171.804532   \n",
       "59   147.760822  25165.145528   \n",
       "60    80.897624  13717.810078   \n",
       "\n",
       "                                        joined_tweets  compound  positive  \\\n",
       "56  Can’t believe it took so long for people in th...   -0.9944     0.083   \n",
       "57  @FinesseEness Recommended study material for t...   -0.9940     0.087   \n",
       "58  @Uniswap @AaveAave @compoundfinance @MakerDAO ...   -0.9954     0.077   \n",
       "59  @ColinWier @ThatYumiMa At least the worst song...    0.9998     0.108   \n",
       "60  @lilyjanesmarkle @ZFiendy @beautifulrobot @Ivo...    0.9993     0.096   \n",
       "\n",
       "    negative  neutral  polarity  subjectivity  price_change  \n",
       "56     0.094    0.823  0.038651      0.465325           0.0  \n",
       "57     0.097    0.816  0.038002      0.415383           1.0  \n",
       "58     0.084    0.838  0.041573      0.419930           0.0  \n",
       "59     0.051    0.841  0.050538      0.436137           0.0  \n",
       "60     0.075    0.829  0.032106      0.428238           1.0  "
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_coin_data[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Train Neural Net on Dataset (Attempt 1)\n",
    "---\n",
    "\n",
    "\n",
    "### What to do next:\n",
    "* Probably attempt it differently. Outcomes are horrid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proper Implementation: SVM With Sentiment Analysis\n",
    "---\n",
    "Yeah the last one wasn't good. This one is ight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>Volume USD</th>\n",
       "      <th>compound</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>price_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>166.58</td>\n",
       "      <td>168.31</td>\n",
       "      <td>165.97</td>\n",
       "      <td>165.97</td>\n",
       "      <td>15303.337894</td>\n",
       "      <td>-0.9944</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.038651</td>\n",
       "      <td>0.465325</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>166.00</td>\n",
       "      <td>168.02</td>\n",
       "      <td>166.00</td>\n",
       "      <td>167.64</td>\n",
       "      <td>13429.914508</td>\n",
       "      <td>-0.9940</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.038002</td>\n",
       "      <td>0.415383</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>169.74</td>\n",
       "      <td>170.29</td>\n",
       "      <td>164.42</td>\n",
       "      <td>166.01</td>\n",
       "      <td>33171.804532</td>\n",
       "      <td>-0.9954</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.041573</td>\n",
       "      <td>0.419930</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>170.59</td>\n",
       "      <td>171.04</td>\n",
       "      <td>168.92</td>\n",
       "      <td>170.31</td>\n",
       "      <td>25165.145528</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.050538</td>\n",
       "      <td>0.436137</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>169.35</td>\n",
       "      <td>169.93</td>\n",
       "      <td>168.78</td>\n",
       "      <td>169.57</td>\n",
       "      <td>13717.810078</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.032106</td>\n",
       "      <td>0.428238</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>198.44</td>\n",
       "      <td>198.44</td>\n",
       "      <td>197.38</td>\n",
       "      <td>198.23</td>\n",
       "      <td>63763.804406</td>\n",
       "      <td>0.9889</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.034420</td>\n",
       "      <td>0.505499</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>199.76</td>\n",
       "      <td>199.97</td>\n",
       "      <td>196.41</td>\n",
       "      <td>197.18</td>\n",
       "      <td>103620.568202</td>\n",
       "      <td>0.9266</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.067508</td>\n",
       "      <td>0.389942</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>196.94</td>\n",
       "      <td>197.98</td>\n",
       "      <td>193.62</td>\n",
       "      <td>197.98</td>\n",
       "      <td>86537.507634</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.109864</td>\n",
       "      <td>0.459019</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>193.67</td>\n",
       "      <td>193.83</td>\n",
       "      <td>193.21</td>\n",
       "      <td>193.83</td>\n",
       "      <td>11150.428138</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.038491</td>\n",
       "      <td>0.425467</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>236.79</td>\n",
       "      <td>236.79</td>\n",
       "      <td>235.36</td>\n",
       "      <td>235.56</td>\n",
       "      <td>419.296800</td>\n",
       "      <td>-0.8137</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.005424</td>\n",
       "      <td>0.354840</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       open    high     low   close     Volume USD  compound  positive  \\\n",
       "56   166.58  168.31  165.97  165.97   15303.337894   -0.9944     0.083   \n",
       "57   166.00  168.02  166.00  167.64   13429.914508   -0.9940     0.087   \n",
       "58   169.74  170.29  164.42  166.01   33171.804532   -0.9954     0.077   \n",
       "59   170.59  171.04  168.92  170.31   25165.145528    0.9998     0.108   \n",
       "60   169.35  169.93  168.78  169.57   13717.810078    0.9993     0.096   \n",
       "..      ...     ...     ...     ...            ...       ...       ...   \n",
       "196  198.44  198.44  197.38  198.23   63763.804406    0.9889     0.092   \n",
       "197  199.76  199.97  196.41  197.18  103620.568202    0.9266     0.070   \n",
       "198  196.94  197.98  193.62  197.98   86537.507634    0.9977     0.096   \n",
       "199  193.67  193.83  193.21  193.83   11150.428138    0.9834     0.065   \n",
       "240  236.79  236.79  235.36  235.56     419.296800   -0.8137     0.061   \n",
       "\n",
       "     negative  neutral  polarity  subjectivity  price_change  \n",
       "56      0.094    0.823  0.038651      0.465325           0.0  \n",
       "57      0.097    0.816  0.038002      0.415383           1.0  \n",
       "58      0.084    0.838  0.041573      0.419930           0.0  \n",
       "59      0.051    0.841  0.050538      0.436137           0.0  \n",
       "60      0.075    0.829  0.032106      0.428238           1.0  \n",
       "..        ...      ...       ...           ...           ...  \n",
       "196     0.059    0.849  0.034420      0.505499           0.0  \n",
       "197     0.057    0.873  0.067508      0.389942           0.0  \n",
       "198     0.056    0.848  0.109864      0.459019           1.0  \n",
       "199     0.050    0.885  0.038491      0.425467           1.0  \n",
       "240     0.069    0.871  0.005424      0.354840           0.0  \n",
       "\n",
       "[67 rows x 12 columns]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are all the columns we actually want to keep for the purposes of training & using the model.\n",
    "model_cols = ['open', 'high', 'low', 'close', 'Volume USD', 'compound', 'positive', 'negative', 'neutral', 'polarity', 'subjectivity', 'price_change']\n",
    "model_df = hourly_coin_data[0][model_cols]\n",
    "\n",
    "# Feature Dataset\n",
    "x = model_df\n",
    "x.drop(['price_change'], axis=1)\n",
    "np.asarray(x)\n",
    "\n",
    "# Target Dataset\n",
    "y = np.array(model_df['price_change'])\n",
    "\n",
    "# split into test & train\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create svm model\n",
    "model = LinearDiscriminantAnalysis().fit(x_train, y_train)\n",
    "predictions = model.predict(x_test)\n",
    "print(classification_report(y_test, predictions))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4c0bf06c6142ddc920bc4833060833a5c39c864bf9bfacfcb217d05e37f17a2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
