{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statements\n",
    "---\n",
    "**Important note:**\n",
    "For some reason tensorflow version and numpy version have dependency conflicts. Need to figure out what version is stable for both of these to work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd                 # Pandas dataframe library\n",
    "import pandas_datareader as pdr     # Pandas datareader that allows me to lookup & store live crypto prices from yahoo finance.\n",
    "import numpy as np                  # Numpy\n",
    "import matplotlib.pyplot as pypl    # Pyplot used to create visuals/graphics based on data \n",
    "import datetime as dt               # Datetime library.\n",
    "import time\n",
    "\n",
    "import glob                         # For changing/finding proper directory\n",
    "import os                           # For changing/finding proper directory (when opening files)\n",
    "\n",
    "import twint                        # Twitter web scraping tool with more features than the regular twitter API\n",
    "import nest_asyncio                 # Import required for twint usage.\n",
    "nest_asyncio.apply()                \n",
    "\n",
    "import re                           # Regex for string cleaning (used for Textblob Sentiment Analysis)\n",
    "from textblob import TextBlob       # Textblob used for sentiment analysis of cleaned data.\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer    # Sentiment analysis tool that works great on determining social media sentiment.\n",
    "from newsapi import NewsApiClient   # NewsApiClient lets me look up/pull news articles relating to specified topics.\n",
    "import requests                     # Used for sending get requests to the NewsAPI client.\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler                          # Scaler used for scaling data (LSTMRNN Implementation)\n",
    "from sklearn.metrics import accuracy_score, classification_report       \n",
    "from sklearn.model_selection import train_test_split                    # Used for splitting data\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis    # Used for implementing SVM\n",
    "import tensorflow as tf                                                 # TF used for LSTMRNN Implmentation\n",
    "from keras.layers import Dense, Dropout, LSTM                           # Dense, dropout & lstm used for creating LSTMRNN \n",
    "from keras.models import Sequential                                     # Important because we're working with Sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in crypto price dataset\n",
    "---\n",
    "Section below reads csv files into pandas dataframes for interacting with. Also compiles list of coin names for twitter searching.\n",
    "\n",
    "### What to do next:\n",
    "* Retrieve Token labels from CSV file for searching by Cashtag on twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unix</th>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>Volume AAVE</th>\n",
       "      <th>Volume USD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1649721600</td>\n",
       "      <td>2022-04-12 00:00:00</td>\n",
       "      <td>AAVE/USD</td>\n",
       "      <td>157.76</td>\n",
       "      <td>159.28</td>\n",
       "      <td>157.76</td>\n",
       "      <td>159.14</td>\n",
       "      <td>73.351666</td>\n",
       "      <td>11673.184137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1649718000</td>\n",
       "      <td>2022-04-11 23:00:00</td>\n",
       "      <td>AAVE/USD</td>\n",
       "      <td>160.55</td>\n",
       "      <td>160.55</td>\n",
       "      <td>157.50</td>\n",
       "      <td>157.68</td>\n",
       "      <td>112.105089</td>\n",
       "      <td>17676.730430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1649714400</td>\n",
       "      <td>2022-04-11 22:00:00</td>\n",
       "      <td>AAVE/USD</td>\n",
       "      <td>156.71</td>\n",
       "      <td>160.42</td>\n",
       "      <td>155.77</td>\n",
       "      <td>159.11</td>\n",
       "      <td>24.787057</td>\n",
       "      <td>3943.868670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1649710800</td>\n",
       "      <td>2022-04-11 21:00:00</td>\n",
       "      <td>AAVE/USD</td>\n",
       "      <td>160.70</td>\n",
       "      <td>160.84</td>\n",
       "      <td>157.81</td>\n",
       "      <td>157.83</td>\n",
       "      <td>310.394127</td>\n",
       "      <td>48989.504990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1649707200</td>\n",
       "      <td>2022-04-11 20:00:00</td>\n",
       "      <td>AAVE/USD</td>\n",
       "      <td>162.31</td>\n",
       "      <td>162.34</td>\n",
       "      <td>160.04</td>\n",
       "      <td>162.34</td>\n",
       "      <td>272.049191</td>\n",
       "      <td>44164.465641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         unix                 date    symbol    open    high     low   close  \\\n",
       "0  1649721600  2022-04-12 00:00:00  AAVE/USD  157.76  159.28  157.76  159.14   \n",
       "1  1649718000  2022-04-11 23:00:00  AAVE/USD  160.55  160.55  157.50  157.68   \n",
       "2  1649714400  2022-04-11 22:00:00  AAVE/USD  156.71  160.42  155.77  159.11   \n",
       "3  1649710800  2022-04-11 21:00:00  AAVE/USD  160.70  160.84  157.81  157.83   \n",
       "4  1649707200  2022-04-11 20:00:00  AAVE/USD  162.31  162.34  160.04  162.34   \n",
       "\n",
       "   Volume AAVE    Volume USD  \n",
       "0    73.351666  11673.184137  \n",
       "1   112.105089  17676.730430  \n",
       "2    24.787057   3943.868670  \n",
       "3   310.394127  48989.504990  \n",
       "4   272.049191  44164.465641  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'c:\\Users\\Brand\\OneDrive\\Documents\\GitHub\\CryptoPredictionTool\\prices\\DailyPrices'\n",
    "extension = 'csv'\n",
    "os.chdir(path)\n",
    "daily_csv_files = glob.glob('*.{}'.format(extension))\n",
    "\n",
    "\n",
    "path = r'c:\\Users\\Brand\\OneDrive\\Documents\\GitHub\\CryptoPredictionTool\\prices\\HourlyPrices'\n",
    "os.chdir(path)\n",
    "hourly_csv_files = glob.glob('*.{}'.format(extension))\n",
    "\n",
    "# Compile list of all coin names for searching on twitter later\n",
    "daily_coins = []\n",
    "hourly_coins = []\n",
    "\n",
    "for coin in daily_csv_files:\n",
    "    vals = coin.split(\"_\")\n",
    "    coin_name = vals[1][:-4]\n",
    "    daily_coins.append(coin_name)\n",
    "\n",
    "for coin in hourly_csv_files:\n",
    "    vals = coin.split(\"_\")\n",
    "    coin_name = vals[0]\n",
    "    hourly_coins.append(coin_name)\n",
    "\n",
    "# compile list of pandas dataframes for use later.\n",
    "hourly_coin_data = []\n",
    "\n",
    "for file in hourly_csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    hourly_coin_data.append(df)\n",
    "\n",
    "hourly_coin_data[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE:* The cell below is for reading in the Bitcoin tweets dataset from Kaggle. (https://www.kaggle.com/datasets/kaushiksuresh147/bitcoin-tweets)\n",
    "This datset kinda sucks though. For a few reasons:\n",
    "* Firstly, its tweets span 1.5 years but are only from 43 total days, making it inconsistent to use with Sequential data, like the price history.\n",
    "* Secondly, it has some values in impropere columns (namely tag values in the date column) which have to be manually removed.\n",
    "* Lastly, its huge. 280k tweets. Which at first seems great, but being that the sample size itself is incredibly sparse in terms of date-span, this leads to problems with implementation. \n",
    "\n",
    "I'll leave it here in a cell in case I decide to use it later, but for now, it doesn't apply to this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!! BELOW IS THE LOGIC FOR READING IN THE TWEETS FROM THE BITCOIN TWEET KAGGLE DATASET !!!\n",
    "# Note: This dataset kinda sucks. It has some values in the \n",
    "\n",
    "# Logic for reading in Bitcoin tweets dataset.\n",
    "# btc_tweets = pd.read_csv('../bitcoin_tweets/Bitcoin_tweets.csv')\n",
    "# btc_tweets.drop([64943], axis=0, inplace=True)\n",
    "# btc_tweets.drop([137068], axis=0, inplace=True)\n",
    "# btc_tweets.drop([180575], axis=0, inplace=True)\n",
    "\n",
    "# btc_tweets.drop(btc_tweets.index[100000:len(btc_tweets)], inplace=True)\n",
    "# btc_tweets.drop(columns=['user_name', 'user_location', 'user_description', 'user_created', 'user_followers', 'user_friends', 'user_favourites', 'user_verified', 'source', 'is_retweet'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'c:\\Users\\WaKaBurd\\Documents\\GitHub\\CryptoPredictionTool\\search_results'\n",
    "for coin in hourly_coins: \n",
    "        os.chdir(path)\n",
    "        os.mkdir(coin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Twitter for data on all coins supplied by dataset\n",
    "---\n",
    "Below section of code searches through twitter using keywords. Uses sift_tweet() function to remove all unnecessary characters, links, emojis & words from tweets. Also uses Textblob to append polarity column to pandas df for tracking sentiment of tweets.\n",
    "\n",
    "### What to do next:\n",
    "* Search twitter based on Cashtags & Hashtags\n",
    "* Configure Twint with Google translater so I can translate tweets from non-english langauges to english. (Need to create ticket for this in Github)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing twitter search for coin: AAVE\n",
      "searching 2022-04-10 to 2022-04-12\n",
      "performing twitter search for coin: AVAX\n",
      "searching 2022-04-10 to 2022-04-12\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "performing twitter search for coin: BCH\n",
      "searching 2022-04-10 to 2022-04-12\n",
      "performing twitter search for coin: BTC\n",
      "searching 2022-04-10 to 2022-04-12\n",
      "performing twitter search for coin: ETH\n",
      "searching 2022-04-10 to 2022-04-12\n"
     ]
    }
   ],
   "source": [
    "os.chdir(r'C:\\Users\\Brand\\OneDrive\\Documents\\GitHub\\CryptoPredictionTool\\archive')\n",
    "\n",
    "# Need to create function for cleaning the tweets.\n",
    "def sift_tweet(tweet, stop_words):\n",
    "    cleaned_tweet = tweet\n",
    "    cleaned_tweet = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",tweet) # regex to remove all @userame, emojis, and links from tweets.\n",
    "    for word in cleaned_tweet:\n",
    "        if word in stop_words: cleaned_tweet.replace(word, '')\n",
    "    return cleaned_tweet\n",
    "\n",
    "# Function for iterating through coins list and storing findings in .csv files\n",
    "def search_coins(coins):\n",
    "    important_cols = ['date', 'created_at', 'tweet']\n",
    "    coin_counter = 0\n",
    "    \n",
    "    for coin in coins:\n",
    "        path = r'c:\\Users\\Brand\\OneDrive\\Documents\\GitHub\\CryptoPredictionTool\\search_results'\n",
    "        os.chdir(path)\n",
    "        #os.mkdir(coin)\n",
    "        os.chdir(coin)\n",
    "        \n",
    "        print('performing twitter search for coin:', coin)\n",
    "        \n",
    "        # Comprises list of hours from our hourly dataset so we can run twint searches on those hours only.\n",
    "        hours = []\n",
    "        for hour in hourly_coin_data[coin_counter]['date']:\n",
    "            hours.append(hour)\n",
    "        \n",
    "        for i in range(len(hours)-1):\n",
    "            \n",
    "            # to_date = hours[i]\n",
    "            # from_date = hours[i+1]\n",
    "           \n",
    "            from_date = '2022-04-10'\n",
    "            to_date = '2022-04-12'\n",
    "            #coin = \"Bitcoin\"\n",
    "            print(f'searching {from_date} to {to_date}')\n",
    "            \n",
    "            c = twint.Config()\n",
    "            c.Limit = 3000\n",
    "            c.Lang = \"en\"\n",
    "            c.Pandas = True\n",
    "            c.Search = coin\n",
    "            c.Hide_output = True\n",
    "            c.Since = from_date\n",
    "            c.Until = to_date\n",
    "            c.Store_csv = True\n",
    "            c.Output = coin + '_' + from_date + '_' + to_date + '_search_result.csv'\n",
    "            twint.run.Search(c)\n",
    "            coin_df = twint.storage.panda.Tweets_df\n",
    "            break\n",
    "            \n",
    "            # important for when twint fails to find tweets based on currency.\n",
    "            # if coin_df.empty:\n",
    "            #     print('no results, moving on...')\n",
    "            #     break\n",
    "            \n",
    "        \n",
    "        \n",
    "            # Processing twitter live tweets\n",
    "            # coin_df['Processed Tweet'] = coin_df['tweet'].apply(lambda x: sift_tweet(x.lower(), stopwords))       # Lambda function for creating processed tweets in Coin Dataframe \n",
    "            # coin_df['Polarity'] = coin_df['Processed Tweet'].apply(lambda x: TextBlob(x).sentiment[0])            # Lambda function for creating Polarity value in Coin Dataframe using Textblob\n",
    "            \n",
    "            \n",
    "            # from_date = re.sub(' ','-',from_date)\n",
    "            # from_date = re.sub(':','-',from_date)\n",
    "            \n",
    "            \n",
    "            #coin_df.to_csv(from_date + '-' + to_date + '.csv')\n",
    "\n",
    "# btc_tweets.text=btc_tweets.text.astype(str)\n",
    "# btc_tweets['Processed Tweet'] = btc_tweets['text'].apply(lambda x: sift_tweet(x.lower(), stopwords)) \n",
    "# btc_tweets['Polarity/Subjectivity'] = btc_tweets['Processed Tweet'].apply(lambda x: TextBlob(x).sentiment)            \n",
    "\n",
    "# btc_tweets\n",
    "\n",
    "search_coins(hourly_coins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.chdir(r'C:\\Users\\Brand\\OneDrive\\Documents\\GitHub\\CryptoPredictionTool\\search_results')\n",
    "tweet_pds = []\n",
    "grouped_tweets = []\n",
    "\n",
    "# Read Tweets into a CSV\n",
    "for coin in hourly_coins:\n",
    "    \n",
    "    os.chdir(r'C:\\Users\\Brand\\OneDrive\\Documents\\GitHub\\CryptoPredictionTool\\search_results')\n",
    "    os.chdir(coin)\n",
    "    csv_names = glob.glob('*.{}'.format(extension))\n",
    "    coin_pds = []\n",
    "    for file in csv_names:\n",
    "        tweet_pd = pd.read_csv(file)\n",
    "        tweet_pd.sort_values(by='date')\n",
    "        coin_pds.append(tweet_pd)\n",
    "    tweet_pds.append(coin_pds)\n",
    "\n",
    "\n",
    "tweet_pds[0][0]['date'] = pd.to_datetime(tweet_pds[0][0]['date'])\n",
    "hourly_coin_data[0]['date'] = pd.to_datetime(hourly_coin_data[0]['date'])\n",
    "hourly_coin_data[0]['joined_tweets'] = np.nan\n",
    "time_and_tweets = []\n",
    "\n",
    "for day in range(1,15):\n",
    "    for hour in range(24):\n",
    "        tweet_time = []\n",
    "        combined_tweets = []\n",
    "        tweet_time_mask = (tweet_pds[0][0]['date'].dt.hour >= hour) & (tweet_pds[0][0]['date'].dt.hour < hour + 1) & \\\n",
    "                    (tweet_pds[0][0]['date'].dt.day >= day ) & (tweet_pds[0][0]['date'].dt.day < day + 1)\n",
    "        price_time_mask = (hourly_coin_data[0]['date'].dt.hour >= hour) & (hourly_coin_data[0]['date'].dt.hour < hour + 1) & \\\n",
    "                    (hourly_coin_data[0]['date'].dt.day >= day ) & (hourly_coin_data[0]['date'].dt.day < day + 1)\n",
    "\n",
    "\n",
    "        hour_view = tweet_pds[0][0][tweet_time_mask]\n",
    "        if hour_view.empty:\n",
    "            continue\n",
    "        \n",
    "        joined_tweets = ' '.join(hour_view['tweet'])\n",
    "\n",
    "        index = hourly_coin_data[0][price_time_mask].index\n",
    "        for i in index:\n",
    "            hourly_coin_data[0].at[i,'joined_tweets'] = joined_tweets\n",
    "        \n",
    "        continue\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        time_and_tweets.append(joined_tweets)\n",
    "\n",
    "        # This groups the tweets by hour, then outputs how many tweets there are in that hour.\n",
    "        #grouped_tweets = tweet_pds[0][0].groupby(pd.Grouper(key='date', freq='60Min')).count()\n",
    "\n",
    "\n",
    "        #grouped_tweets['joined_tweets'] = joined_tweets\n",
    "        #print(grouped_tweets)\n",
    "# print(tweet_pds[0][0]['date'])\n",
    "# def split_tweets(tweets):\n",
    "#     return\n",
    "hourly_coin_data[0] = hourly_coin_data[0][hourly_coin_data[0]['joined_tweets'].notna()]\n",
    "\n",
    "print(len(hourly_coin_data[0]))\n",
    "#pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unix</th>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>Volume AAVE</th>\n",
       "      <th>Volume USD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1649721600</td>\n",
       "      <td>2022-04-12 00:00:00</td>\n",
       "      <td>AAVE/USD</td>\n",
       "      <td>157.76</td>\n",
       "      <td>159.28</td>\n",
       "      <td>157.76</td>\n",
       "      <td>159.14</td>\n",
       "      <td>73.351666</td>\n",
       "      <td>11673.184137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1649718000</td>\n",
       "      <td>2022-04-11 23:00:00</td>\n",
       "      <td>AAVE/USD</td>\n",
       "      <td>160.55</td>\n",
       "      <td>160.55</td>\n",
       "      <td>157.50</td>\n",
       "      <td>157.68</td>\n",
       "      <td>112.105089</td>\n",
       "      <td>17676.730430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1649714400</td>\n",
       "      <td>2022-04-11 22:00:00</td>\n",
       "      <td>AAVE/USD</td>\n",
       "      <td>156.71</td>\n",
       "      <td>160.42</td>\n",
       "      <td>155.77</td>\n",
       "      <td>159.11</td>\n",
       "      <td>24.787057</td>\n",
       "      <td>3943.868670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1649710800</td>\n",
       "      <td>2022-04-11 21:00:00</td>\n",
       "      <td>AAVE/USD</td>\n",
       "      <td>160.70</td>\n",
       "      <td>160.84</td>\n",
       "      <td>157.81</td>\n",
       "      <td>157.83</td>\n",
       "      <td>310.394127</td>\n",
       "      <td>48989.504990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1649707200</td>\n",
       "      <td>2022-04-11 20:00:00</td>\n",
       "      <td>AAVE/USD</td>\n",
       "      <td>162.31</td>\n",
       "      <td>162.34</td>\n",
       "      <td>160.04</td>\n",
       "      <td>162.34</td>\n",
       "      <td>272.049191</td>\n",
       "      <td>44164.465641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         unix                date    symbol    open    high     low   close  \\\n",
       "0  1649721600 2022-04-12 00:00:00  AAVE/USD  157.76  159.28  157.76  159.14   \n",
       "1  1649718000 2022-04-11 23:00:00  AAVE/USD  160.55  160.55  157.50  157.68   \n",
       "2  1649714400 2022-04-11 22:00:00  AAVE/USD  156.71  160.42  155.77  159.11   \n",
       "3  1649710800 2022-04-11 21:00:00  AAVE/USD  160.70  160.84  157.81  157.83   \n",
       "4  1649707200 2022-04-11 20:00:00  AAVE/USD  162.31  162.34  160.04  162.34   \n",
       "\n",
       "   Volume AAVE    Volume USD  \n",
       "0    73.351666  11673.184137  \n",
       "1   112.105089  17676.730430  \n",
       "2    24.787057   3943.868670  \n",
       "3   310.394127  48989.504990  \n",
       "4   272.049191  44164.465641  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_coin_data[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Train Neural Net on Dataset (Attempt 1)\n",
    "---\n",
    "\n",
    "\n",
    "### What to do next:\n",
    "* Probably attempt it differently. Outcomes are horrid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proper Implementation: SVM With Sentiment Analysis\n",
    "---\n",
    "Yeah the last one wasn't good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# btc_tweets['Compound'] = compound\n",
    "# btc_tweets['Positive'] = pos\n",
    "# btc_tweets['Negative'] = neg\n",
    "# btc_tweets['Neutral'] = neu\n",
    "\n",
    "# These are all the columns we actually want to keep for the purposes of training & using the model.\n",
    "model_cols = ['Label','Date', 'High', 'Low', 'Open', 'Close', 'Volume', 'Compound', 'Positive', 'Negative', 'Neutral']\n",
    "\n",
    "btc_prices['Date'] = pd.to_datetime(btc_prices['Date'])\n",
    "#btc_prices['Date'] = btc_prices['Date'].dt.date\n",
    "btc_tweets['date'] = pd.to_datetime(btc_tweets['date'])\n",
    "#btc_tweets['date'] = btc_tweets['date'].dt.date\n",
    "\n",
    "#Create function to get Sentiment Scores\n",
    "def getSIA(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment\n",
    "\n",
    "#Get sentiment scores for each day\n",
    "compound, pos, neg, neu = [], [], [], []\n",
    "SIA = 0\n",
    "\n",
    "for i in range(0, len(btc_tweets['text'])):\n",
    "    SIA = getSIA(btc_tweets['text'][i])\n",
    "    compound.append(SIA['compound'])                    # Score representing sum(lexicon ratings)\n",
    "    pos.append(SIA['pos'])\n",
    "    neg.append(SIA['neg'])\n",
    "    neu.append(SIA['neu'])\n",
    "\n",
    "btc_tweets['Compound'] = compound\n",
    "btc_tweets['Positive'] = pos\n",
    "btc_tweets['Negative'] = neg\n",
    "btc_tweets['Neutral'] = neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [date, text, hashtags]\n",
      "Index: []\n",
      "2021-02-05\n",
      "2021-02-06\n",
      "2021-02-07\n",
      "2021-02-08\n",
      "2021-02-09\n",
      "2021-02-10\n",
      "2021-02-13\n",
      "2021-02-14\n",
      "2021-02-15\n",
      "2021-02-18\n",
      "2021-02-19\n",
      "2021-02-22\n",
      "2021-02-28\n",
      "2021-03-11\n",
      "2021-03-12\n",
      "2021-04-05\n",
      "2021-04-06\n",
      "2021-04-07\n",
      "2021-04-08\n",
      "2021-04-09\n",
      "2021-04-10\n",
      "2021-04-11\n",
      "2021-04-12\n",
      "2021-04-17\n",
      "2021-04-18\n",
      "2021-04-19\n",
      "2021-04-20\n",
      "2021-04-21\n",
      "2021-04-22\n",
      "2021-04-23\n",
      "2021-04-24\n",
      "2021-05-25\n",
      "2021-05-26\n",
      "2021-05-27\n",
      "2021-05-28\n",
      "2021-05-29\n",
      "2021-06-20\n",
      "2021-06-21\n",
      "2021-06-22\n",
      "2021-06-23\n",
      "2021-07-04\n",
      "2021-07-05\n",
      "NaT\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "print(btc_tweets.loc[btc_tweets['date'] == '[\\'YieldFarming\\', \\'Airdrop\\', \\'Binance\\', \\'Bitcoin\\', \\'pancakeswap\\', \\'BNB\\', \\'cryptocurrency\\', \\'DeFi\\', \\'BTC\\', \\'BinanceSmartChain\\', \\'BSC\\', \\'pufferswap\\', \\'DeFi\\', \\'bsc\\', \\'bnb\\', \\'bitcoin\\', \\'cryto\\', \\'Airdrop\\', \\'Airdrop\\']'])\n",
    "#btc_tweets.drop([64943], axis=0, inplace=True)\n",
    "#btc_tweets.drop([137068], axis=0, inplace=True)\n",
    "#btc_tweets.drop([180575], axis=0, inplace=True)\n",
    "\n",
    "btc_tweets['date'] = pd.to_datetime(btc_tweets['date'])\n",
    "btc_tweets['date'] = btc_tweets['date'].dt.date\n",
    "\n",
    "btc_tweets = btc_tweets.sort_values(by='date', ascending=True)\n",
    "dataset_dates = btc_tweets['date'].unique()\n",
    "num_days = 0\n",
    "for date in dataset_dates:\n",
    "    print(date)\n",
    "    num_days += 1\n",
    "\n",
    "print(num_days)\n",
    "\n",
    "split_date = datetime.date(2021,2,10)\n",
    "\n",
    "# train = btc_tweets.loc[btc_tweets['date'] == split_date]\n",
    "# test = btc_tweets.loc[btc_tweets['date'] >= split_date]\n",
    "\n",
    "\n",
    "#print(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newsapi example function call\n",
    "---\n",
    "* This will be used in the case that I decide to try to integrate news posts for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>totalResults</th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ok</td>\n",
       "      <td>11728</td>\n",
       "      <td>{'source': {'id': 'engadget', 'name': 'Engadge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok</td>\n",
       "      <td>11728</td>\n",
       "      <td>{'source': {'id': 'engadget', 'name': 'Engadge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ok</td>\n",
       "      <td>11728</td>\n",
       "      <td>{'source': {'id': 'wired', 'name': 'Wired'}, '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  status  totalResults                                           articles\n",
       "0     ok         11728  {'source': {'id': 'engadget', 'name': 'Engadge...\n",
       "1     ok         11728  {'source': {'id': 'engadget', 'name': 'Engadge...\n",
       "2     ok         11728  {'source': {'id': 'wired', 'name': 'Wired'}, '..."
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://newsapi.org/v2/everything?'\n",
    "parameters = {\n",
    "    'q': 'bitcoin',\n",
    "    'from': '2022-03-12',\n",
    "    'to': '2022-04-11',\n",
    "    'sortBy': 'popularity',\n",
    "    'apiKey': 'f2162fa3a1ed4fa29bb14cb6a737be55'\n",
    "}\n",
    "\n",
    "response = requests.get(url, parameters)\n",
    "headlines = pd.DataFrame(response.json())\n",
    "\n",
    "headlines.head(3)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4c0bf06c6142ddc920bc4833060833a5c39c864bf9bfacfcb217d05e37f17a2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
