{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statements\n",
    "---\n",
    "**Important note:**\n",
    "For some reason tensorflow version and numpy version have dependency conflicts. Need to figure out what version is stable for both of these to work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd                 # Pandas dataframe library\n",
    "import pandas_datareader as pdr     # Pandas datareader that allows me to lookup & store live crypto prices from yahoo finance.\n",
    "import numpy as np                  # Numpy\n",
    "import matplotlib.pyplot as pypl    # Pyplot used to create visuals/graphics based on data \n",
    "from alpha_vantage.timeseries import TimeSeries     # Library used for pulling live price data from alphavantage api\n",
    "\n",
    "from datetime import datetime, timedelta, timezone             # Datetime library.\n",
    "import pytz\n",
    "import json\n",
    "import csv\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=ResourceWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import glob                         # For changing/finding proper directory\n",
    "import os                           # For changing/finding proper directory (when opening files)\n",
    "import requests\n",
    "import twint                        # Twitter web scraping tool with more features than the regular twitter API\n",
    "import nest_asyncio                 # Import required for twint usage.\n",
    "nest_asyncio.apply()                \n",
    "\n",
    "import re                           # Regex for string cleaning (used for Textblob Sentiment Analysis)\n",
    "from textblob import TextBlob       # Textblob used for sentiment analysis of cleaned data.\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer    # Sentiment analysis tool that works great on determining social media sentiment.\n",
    "from newsapi import NewsApiClient   # NewsApiClient lets me look up/pull news articles relating to specified topics.\n",
    "import requests                     # Used for sending get requests to the NewsAPI client.\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler                          # Scaler used for scaling data (LSTMRNN Implementation)\n",
    "from sklearn.metrics import accuracy_score, classification_report       \n",
    "from sklearn.model_selection import train_test_split                    # Used for splitting data\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis    # Used for implementing SVM\n",
    "import tensorflow as tf                                                 # TF used for LSTMRNN Implmentation\n",
    "from keras.layers import Dense, Dropout, LSTM                           # Dense, dropout & lstm used for creating LSTMRNN \n",
    "from keras.models import Sequential                                     # Important because we're working with Sequential data.\n",
    "\n",
    "os.chdir(r'C:\\Users\\WaKaBurd\\Documents\\GitHub\\CryptoPredictionTool\\archive')\n",
    "stopwords_file = open(\"stopwords.txt\", \"r+\")\n",
    "stopwords = list(stopwords_file.read().split('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in crypto price dataset\n",
    "---\n",
    "Section below reads csv files into pandas dataframes for interacting with. Also compiles list of coin names for twitter searching.\n",
    "\n",
    "### What to do next:\n",
    "* Retrieve Token labels from CSV file for searching by Cashtag on twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'c:\\Users\\WaKaBurd\\Documents\\GitHub\\CryptoPredictionTool\\prices\\DailyPrices'\n",
    "extension = 'csv'\n",
    "os.chdir(path)\n",
    "daily_csv_files = glob.glob('*.{}'.format(extension))\n",
    "\n",
    "\n",
    "path = r'c:\\Users\\WaKaBurd\\Documents\\GitHub\\CryptoPredictionTool\\prices\\HourlyPrices'\n",
    "os.chdir(path)\n",
    "hourly_csv_files = glob.glob('*.{}'.format(extension))\n",
    "\n",
    "# Compile list of all coin names for searching on twitter later\n",
    "daily_coins = []\n",
    "hourly_coins = []\n",
    "\n",
    "for coin in daily_csv_files:\n",
    "    vals = coin.split(\"_\")\n",
    "    coin_name = vals[1][:-4]\n",
    "    daily_coins.append(coin_name)\n",
    "\n",
    "for coin in hourly_csv_files:\n",
    "    vals = coin.split(\"_\")\n",
    "    coin_name = vals[0]\n",
    "    hourly_coins.append(coin_name)\n",
    "\n",
    "# compile list of pandas dataframes for use later.\n",
    "hourly_coin_data = []\n",
    "\n",
    "for file in hourly_csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    hourly_coin_data.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           unix                 date    symbol    open    high     low  \\\n",
      "0    1650931200  2022-04-26 00:00:00  AAVE/USD  169.23  170.68  168.38   \n",
      "1    1650927600  2022-04-25 23:00:00  AAVE/USD  169.68  170.57  169.68   \n",
      "2    1650924000  2022-04-25 22:00:00  AAVE/USD  168.21  169.51  168.21   \n",
      "3    1650920400  2022-04-25 21:00:00  AAVE/USD  168.46  168.46  168.46   \n",
      "4    1650916800  2022-04-25 20:00:00  AAVE/USD  169.83  169.90  169.83   \n",
      "..          ...                  ...       ...     ...     ...     ...   \n",
      "575  1648861200  2022-04-02 01:00:00  AAVE/USD  247.31  247.31  245.64   \n",
      "576  1648857600  2022-04-02 00:00:00  AAVE/USD  242.29  256.74  242.29   \n",
      "577  1648854000  2022-04-01 23:00:00  AAVE/USD  246.88  246.88  244.55   \n",
      "578  1648850400  2022-04-01 22:00:00  AAVE/USD  250.60  250.60  246.39   \n",
      "579  1648846800  2022-04-01 21:00:00  AAVE/USD  252.84  254.52  249.91   \n",
      "\n",
      "      close  Volume AAVE    Volume USD  \n",
      "0    169.48   174.216470  29526.207266  \n",
      "1    170.10     1.814157    308.588094  \n",
      "2    169.51     0.383062     64.932829  \n",
      "3    168.46     3.677922    619.582701  \n",
      "4    169.90    11.501266   1954.065088  \n",
      "..      ...          ...           ...  \n",
      "575  246.04    37.644060   9261.944540  \n",
      "576  251.79    55.034548  13857.148899  \n",
      "577  246.07    36.815549   9059.202069  \n",
      "578  247.57    19.513300   4830.907785  \n",
      "579  250.53    86.272691  21613.897361  \n",
      "\n",
      "[580 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(hourly_coin_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE:* The cell below is for reading in the Bitcoin tweets dataset from Kaggle. (https://www.kaggle.com/datasets/kaushiksuresh147/bitcoin-tweets)\n",
    "This datset kinda sucks though. For a few reasons:\n",
    "* Firstly, its tweets span 1.5 years but are only from 43 total days, making it inconsistent to use with Sequential data, like the price history.\n",
    "* Secondly, it has some values in impropere columns (namely tag values in the date column) which have to be manually removed.\n",
    "* Lastly, its huge. 280k tweets. Which at first seems great, but being that the sample size itself is incredibly sparse in terms of date-span, this leads to problems with implementation. \n",
    "\n",
    "I'll leave it here in a cell in case I decide to use it later, but for now, it doesn't apply to this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!! BELOW IS THE LOGIC FOR READING IN THE TWEETS FROM THE BITCOIN TWEET KAGGLE DATASET !!!\n",
    "# Note: This dataset kinda sucks. It has some values in the \n",
    "\n",
    "# Logic for reading in Bitcoin tweets dataset.\n",
    "# btc_tweets = pd.read_csv('../bitcoin_tweets/Bitcoin_tweets.csv')\n",
    "# btc_tweets.drop([64943], axis=0, inplace=True)\n",
    "# btc_tweets.drop([137068], axis=0, inplace=True)\n",
    "# btc_tweets.drop([180575], axis=0, inplace=True)\n",
    "\n",
    "# btc_tweets.drop(btc_tweets.index[100000:len(btc_tweets)], inplace=True)\n",
    "# btc_tweets.drop(columns=['user_name', 'user_location', 'user_description', 'user_created', 'user_followers', 'user_friends', 'user_favourites', 'user_verified', 'source', 'is_retweet'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Twitter for data on all coins supplied by dataset\n",
    "---\n",
    "Below section of code searches through twitter using keywords. Uses sift_tweet() function to remove all unnecessary characters, links, emojis & words from tweets. Also uses Textblob to append polarity column to pandas df for tracking sentiment of tweets.\n",
    "\n",
    "### What to do next:\n",
    "* Search twitter based on Cashtags & Hashtags\n",
    "* Configure Twint with Google translater so I can translate tweets from non-english langauges to english. (Need to create ticket for this in Github)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing twitter search for coin: AAVE\n",
      "searching 2022-04-24 to 2022-04-26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:twint.run:Twint:Feed:noData'globalObjects'\n",
      "sleeping for 1.0 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "performing twitter search for coin: AVAX\n",
      "searching 2022-04-24 to 2022-04-26\n",
      "performing twitter search for coin: BCH\n",
      "searching 2022-04-24 to 2022-04-26\n",
      "performing twitter search for coin: BTC\n",
      "searching 2022-04-24 to 2022-04-26\n",
      "performing twitter search for coin: ETH\n",
      "searching 2022-04-24 to 2022-04-26\n"
     ]
    }
   ],
   "source": [
    "# Function for iterating through coins list and storing findings in .csv files\n",
    "def search_coins(coins):\n",
    "    \n",
    "    for coin in coins:\n",
    "        path = r'c:\\Users\\WaKaBurd\\Documents\\GitHub\\CryptoPredictionTool\\search_results'\n",
    "        os.chdir(path)\n",
    "        #os.mkdir(coin)\n",
    "        os.chdir(coin)\n",
    "        \n",
    "        print('performing twitter search for coin:', coin)\n",
    "        \n",
    "        from_date = '2022-04-24'\n",
    "        to_date = '2022-04-26'\n",
    "        #coin = \"Bitcoin\"\n",
    "        print(f'searching {from_date} to {to_date}')\n",
    "        \n",
    "        c = twint.Config()\n",
    "        c.Limit = 3000\n",
    "        c.Lang = \"en\"\n",
    "        c.Pandas = True\n",
    "        c.Search = coin\n",
    "        c.Hide_output = True\n",
    "        c.Since = from_date\n",
    "        c.Until = to_date\n",
    "        c.Store_csv = True\n",
    "        c.Output = coin + '_' + from_date + '_' + to_date + '_search_result.csv'\n",
    "        twint.run.Search(c)\n",
    "\n",
    "\n",
    "# btc_tweets.text=btc_tweets.text.astype(str)\n",
    "# btc_tweets['Processed Tweet'] = btc_tweets['text'].apply(lambda x: sift_tweet(x.lower(), stopwords)) \n",
    "# btc_tweets['Polarity/Subjectivity'] = btc_tweets['Processed Tweet'].apply(lambda x: TextBlob(x).sentiment)            \n",
    "\n",
    "# btc_tweets\n",
    "\n",
    "search_coins(hourly_coins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below chunk is more data pre-processing. \n",
    "I need to modify the dataframe so that it contains both the price information, as well as all of the tweets so I can easily perform sentiment analysis on them using VADER.\n",
    "\n",
    "The code below will read all CSV files that were stored in both the hourly_prices directory (done earlier) as well as the tweets that are searchd for and stored in the search results folders for each currency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to create function for cleaning the tweets so we can derive the subjectivity and polarity using textblob.\n",
    "def sift_tweet(tweet, stop_words):\n",
    "    cleaned_tweet = tweet\n",
    "    cleaned_tweet = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",tweet) # regex to remove all @userame, emojis, and links from tweets.\n",
    "    for word in cleaned_tweet:\n",
    "        if word in stop_words: cleaned_tweet.replace(word, '')\n",
    "    return cleaned_tweet\n",
    "\n",
    "def get_sentiment(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lookin at coin number: 1\n"
     ]
    }
   ],
   "source": [
    "os.chdir(r'C:\\Users\\WaKaBurd\\Documents\\GitHub\\CryptoPredictionTool\\search_results')\n",
    "tweet_pds = []\n",
    "grouped_tweets = []\n",
    "\n",
    "# Read Tweets into a DF from the CSVs\n",
    "for coin in hourly_coins:\n",
    "    \n",
    "    os.chdir(r'C:\\Users\\WaKaBurd\\Documents\\GitHub\\CryptoPredictionTool\\search_results')\n",
    "    os.chdir(coin)\n",
    "    csv_names = glob.glob('*.{}'.format(extension))\n",
    "    coin_pds = []\n",
    "    for file in csv_names:\n",
    "        tweet_pd = pd.read_csv(file)\n",
    "        tweet_pd.sort_values(by='date')\n",
    "        coin_pds.append(tweet_pd)\n",
    "    tweet_pds.append(coin_pds)\n",
    "\n",
    "\n",
    "#for i in range(len(tweet_pds)):\n",
    "# This is just so i can the data i need to train a model for aave and avax. I'll do all 5 when i want to showcase something but for now i only need these 2.\n",
    "for i in range(1):\n",
    "    i = 0\n",
    "    print('lookin at coin number:', i)\n",
    "    hourly_coin_data[i]['date'] = pd.to_datetime(hourly_coin_data[i]['date'])\n",
    "    hourly_coin_data[i]['joined_tweets'] = \"\"\n",
    "    hourly_coin_data[i]['compound'] = 0.0\n",
    "    hourly_coin_data[i]['positive'] = 0.0\n",
    "    hourly_coin_data[i]['negative'] = 0.0\n",
    "    hourly_coin_data[i]['neutral'] = 0.0\n",
    "\n",
    "    #print(hourly_coin_data[i])\n",
    "    for j in range(len(tweet_pds[i])):\n",
    "        tweet_pds[i][j]['created_at'] = pd.to_datetime(tweet_pds[i][j]['created_at'])\n",
    "\n",
    "        for day in range(1,31):\n",
    "            #print('checking day:', day)\n",
    "            for hour in range(24):\n",
    "                tweet_time_mask = (tweet_pds[i][j]['created_at'].dt.hour >= hour) & (tweet_pds[i][j]['created_at'].dt.hour < hour + 1) & \\\n",
    "                            (tweet_pds[i][j]['created_at'].dt.day >= day ) & (tweet_pds[i][j]['created_at'].dt.day < day + 1)\n",
    "                price_time_mask = (hourly_coin_data[i]['date'].dt.hour >= hour) & (hourly_coin_data[i]['date'].dt.hour < hour + 1) & \\\n",
    "                            (hourly_coin_data[i]['date'].dt.day >= day ) & (hourly_coin_data[i]['date'].dt.day < day + 1)\n",
    "\n",
    "                hour_view = tweet_pds[i][j][tweet_time_mask]\n",
    "                if hour_view.empty:\n",
    "                    continue\n",
    "                \n",
    "                hour_view['cleaned_tweet'] = hour_view['tweet'].apply(lambda x: sift_tweet(str(x).lower(), stopwords))\n",
    "\n",
    "                joined_tweets = ' '.join(hour_view['tweet'])\n",
    "                joined_clean_tweets = ' '.join(hour_view['cleaned_tweet'])\n",
    "\n",
    "                SIA = get_sentiment(joined_tweets)\n",
    "                compound = SIA['compound']                    # Score representing sum(lexicon ratings)\n",
    "                pos = SIA['pos']\n",
    "                neg = SIA['neg']\n",
    "                neu = SIA['neu']\n",
    "\n",
    "                index = hourly_coin_data[i][price_time_mask].index\n",
    "                for ind in index:\n",
    "                    hourly_coin_data[i].at[ind,'joined_tweets'] = joined_tweets\n",
    "                    hourly_coin_data[i].at[ind,'polarity'] = TextBlob(joined_clean_tweets).sentiment[0]                # Analyze and store Polarity value in Coin Dataframe using Textblob\n",
    "                    hourly_coin_data[i].at[ind,'subjectivity'] = TextBlob(joined_clean_tweets).sentiment[1]            # Analyze and store Subjectivity value in Coin Dataframe using Textblob\n",
    "                    hourly_coin_data[i].at[ind,'compound'] = compound\n",
    "                    hourly_coin_data[i].at[ind,'positive'] = pos\n",
    "                    hourly_coin_data[i].at[ind,'negative'] = neg\n",
    "                    hourly_coin_data[i].at[ind,'neutral'] = neu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the rows that don't contain a polarity score. The only reason they wouldn't have this would be because they didn't have any tweets stored in their row for that hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, 16)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_coin_data[0] = hourly_coin_data[1][hourly_coin_data[1]['polarity'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through remaining rows and appending a price change label. This label signifies whether or not in that hour the price of the coin went up or down. This is what the model is going to be responsible for predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(hourly_coin_data)):\n",
    "    hourly_coin_data[i].reset_index()\n",
    "    hourly_coin_data[i]['price_change'] = np.nan\n",
    "    for index, row in hourly_coin_data[i].iterrows():\n",
    "        if row.open > row.close:\n",
    "            hourly_coin_data[i].at[index, 'price_change'] = 0\n",
    "        else:\n",
    "            hourly_coin_data[i].at[index, 'price_change'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205, 12)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_coin_data[0] = pd.read_csv('model_df_0.csv')\n",
    "hourly_coin_data[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Train Neural Net on Dataset (Attempt 1)\n",
    "---\n",
    "\n",
    "\n",
    "### What to do next:\n",
    "* Get more data and keep training.\n",
    "* Try SVM implementation when enough data is gathered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proper Implementation: LDA With Sentiment Analysis\n",
    "---\n",
    "Yeah the last one wasn't good. This one is ight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT:\n",
    "---\n",
    "* If you're running each cell in the jupyter notebook, you only need to run the below code cell. \n",
    "\n",
    "* If you're going to try to use the exported model_df_#.csv files that are saved in the hourly_coin_data directory, you need to run the 2nd cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       open    high     low     Volume USD  compound  positive  negative  \\\n",
      "0    169.31  169.78  166.84    4483.910281    0.9920     0.107     0.068   \n",
      "1    170.40  170.70  168.83    3966.333567    0.9164     0.098     0.080   \n",
      "2    169.27  170.87  167.84   14345.829974    0.9834     0.069     0.034   \n",
      "3    171.35  171.35  171.35       0.000000   -0.9548     0.080     0.084   \n",
      "4    171.56  172.03  171.35    2540.947375    0.9974     0.110     0.058   \n",
      "..      ...     ...     ...            ...       ...       ...       ...   \n",
      "200  199.76  199.97  196.41  103620.568202    0.0000     0.000     0.000   \n",
      "201  196.94  197.98  193.62   86537.507634    0.0000     0.000     0.000   \n",
      "202  193.67  193.83  193.21   11150.428138    0.0000     0.000     0.000   \n",
      "203  226.39  229.02  226.14     574.700572   -0.8137     0.061     0.069   \n",
      "204  236.79  236.79  235.36     419.296800    0.0000     0.000     0.000   \n",
      "\n",
      "     neutral  polarity  subjectivity  \n",
      "0      0.825 -0.067892      0.365060  \n",
      "1      0.821  0.018946      0.396833  \n",
      "2      0.898  0.101900      0.487789  \n",
      "3      0.836  0.027616      0.444126  \n",
      "4      0.833  0.016304      0.485740  \n",
      "..       ...       ...           ...  \n",
      "200    0.000  0.067508      0.389942  \n",
      "201    0.000  0.109864      0.459019  \n",
      "202    0.000  0.038491      0.425467  \n",
      "203    0.871  0.005424      0.354840  \n",
      "204    0.000  0.005424      0.354840  \n",
      "\n",
      "[205 rows x 10 columns]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.64      0.78        11\n",
      "         1.0       0.88      1.00      0.94        30\n",
      "\n",
      "    accuracy                           0.90        41\n",
      "   macro avg       0.94      0.82      0.86        41\n",
      "weighted avg       0.91      0.90      0.89        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# These are all the columns we actually want to keep for the purposes of training & using the model.\n",
    "model_cols = ['open', 'high', 'low', 'Volume USD', 'compound', 'positive', 'negative', 'neutral', 'polarity', 'subjectivity', 'price_change']\n",
    "os.chdir(r'C:\\Users\\WaKaBurd\\Documents\\GitHub\\CryptoPredictionTool\\hourly_coin_data')\n",
    "\n",
    "for i in range(1):\n",
    "# for i in range(len(hourly_coin_data)):\n",
    "    \n",
    "    model_df = hourly_coin_data[i][model_cols]\n",
    "    model_df.to_csv(f'model_df_{i}.csv')\n",
    "\n",
    "    # Feature Dataset\n",
    "    x = model_df\n",
    "    # Target Dataset\n",
    "    y = np.array(model_df['price_change'])\n",
    "    x.drop(['price_change'], axis=1, inplace=True)\n",
    "    np.asarray(x)\n",
    "    \n",
    "    print(x)\n",
    "\n",
    "    # split into test & train\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    # Create svm model\n",
    "    model = LinearDiscriminantAnalysis().fit(x_train, y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.64      0.78        11\n",
      "         1.0       0.88      1.00      0.94        30\n",
      "\n",
      "    accuracy                           0.90        41\n",
      "   macro avg       0.94      0.82      0.86        41\n",
      "weighted avg       0.91      0.90      0.89        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# These are all the columns we actually want to keep for the purposes of training & using the model.\n",
    "model_cols = ['open', 'high', 'low', 'Volume USD', 'compound', 'positive', 'negative', 'neutral', 'polarity', 'subjectivity', 'price_change']\n",
    "os.chdir(r'C:\\Users\\WaKaBurd\\Documents\\GitHub\\CryptoPredictionTool\\hourly_coin_data')\n",
    "\n",
    "model_df = pd.read_csv('model_df_0.csv')\n",
    "model_df.drop(['drop_this'], axis=1, inplace=True)\n",
    "\n",
    "# Feature Dataset\n",
    "x = model_df\n",
    "# Target Dataset\n",
    "y = np.array(model_df['price_change'])\n",
    "x.drop(['price_change'], axis=1, inplace=True)\n",
    "np.asarray(x)\n",
    "\n",
    "\n",
    "# split into test & train\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# Create svm model\n",
    "model = LinearDiscriminantAnalysis().fit(x_train, y_train)\n",
    "predictions = model.predict(x_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull data from the last hour to make prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing twitter search for coin: AVAX lang:en\n",
      "searching 2022-04-26 22:52:52 to 2022-04-26 23:52:22\n",
      "Endpoint response code:200\n"
     ]
    }
   ],
   "source": [
    "def send_request(url, headers, params, next_token=None):\n",
    "    params['next_token'] = next_token\n",
    "    response = requests.request('GET', url, headers=headers, params=params)\n",
    "    print('Endpoint response code:' + str(response.status_code))\n",
    "    if (response.status_code != 200):\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "def pull_live_tweets(coin):\n",
    "\n",
    "    # Pull tweets from the last hour\n",
    "    path = r'c:\\Users\\WaKaBurd\\Documents\\GitHub\\CryptoPredictionTool\\predicted_trends'\n",
    "    os.chdir(path)\n",
    "    #os.chdir(coin)\n",
    "\n",
    "    print('performing twitter search for coin:', coin)\n",
    "\n",
    "    # 1 hour ago\n",
    "    from_date = datetime.now(timezone.utc) - timedelta(hours = 1)\n",
    "    to_date = datetime.now(timezone.utc) + timedelta(seconds=-30)\n",
    "    \n",
    "    iso_from_date = from_date.isoformat()\n",
    "    iso_to_date = to_date.isoformat()\n",
    "\n",
    "    from_date = from_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    to_date = to_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    print(f'searching {from_date} to {to_date}')\n",
    "    \n",
    "    bearer_token = 'AAAAAAAAAAAAAAAAAAAAAJwBbgEAAAAAyi3tWb4jDN72EZqz6dcWgOIizuc%3DsC3xrWGrxPCwiKwqy2fINUgJDs2qKaZNlITIIy75Pss1oiMeTN'\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer {}\".format(bearer_token)\n",
    "    }\n",
    "\n",
    "    url = 'https://api.twitter.com/2/tweets/search/recent'\n",
    "\n",
    "    params = {\n",
    "        'query': coin,\n",
    "        'start_time': iso_from_date,\n",
    "        'end_time': iso_to_date,\n",
    "        'max_results': 100,\n",
    "        'next_token':{}\n",
    "    }\n",
    "\n",
    "    json_response = send_request(url, headers, params)\n",
    "    return json_response\n",
    "\n",
    "# Pull tweets on topic from last 30 minutes\n",
    "fetched_tweets = pull_live_tweets('AVAX lang:en')\n",
    "fetched_tweets_df = pd.DataFrame(fetched_tweets['data'])\n",
    "fetched_tweets_df.to_csv('recently_fetched_tweets.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00834971 0.99165029]\n"
     ]
    }
   ],
   "source": [
    "# Pull financial data from yahoo finance for the current hour\n",
    "# Uses AlphaVantage API with their CRYPTO_INTRADAY endpoint.\n",
    "\n",
    "av_api_key = 'GD982KLZ6PZ69GQ0'\n",
    "path = r'c:\\Users\\WaKaBurd\\Documents\\GitHub\\CryptoPredictionTool\\prices\\LivePrices'\n",
    "os.chdir(path)\n",
    "\n",
    "def get_prices(coin):\n",
    "    url = f'https://www.alphavantage.co/query?function=CRYPTO_INTRADAY&symbol={coin}&market=USD&interval=1min&apikey={av_api_key}&datatype=csv'\n",
    "    req = requests.get(url)\n",
    "    data = req.content\n",
    "    csv_file = open(f'{coin}_prices.csv','wb')\n",
    "    csv_file.write(data)\n",
    "    csv_file.close()\n",
    "    return\n",
    "\n",
    "get_prices('AVAX')  # Get the prices from the specified coin\n",
    "\n",
    "# format that data into a dataframe\n",
    "live_prices = pd.read_csv('AVAX_prices.csv')    # read in live prices csv\n",
    "kept_prices = live_prices.head(60)              # keep only the last 60 minutes.\n",
    "high = kept_prices['high'].max(axis=0)       # Find the max value in the last 60 minutes\n",
    "low = kept_prices['low'].min(axis=0)        # find the lowesst value in the last 60 minutes\n",
    "open_price = kept_prices['open'].values[59]                 # Price from 60 minutes ago. (opening price of the last hour) \n",
    "volume = kept_prices['volume'].sum(axis=0)      # summate the total volume traded from the last hour\n",
    "\n",
    "live_coin_data = pd.DataFrame([[open_price, high, low, volume]], columns =['open', 'high', 'low', 'volume'])\n",
    "\n",
    "# Run textblob on tweets for polarity & subjectivity\n",
    "combined_tweets = ' '.join(fetched_tweets_df['text'])\n",
    "\n",
    "# Clean tweet so we can use textblob on it.\n",
    "fetched_tweets_df['cleaned_tweet'] = fetched_tweets_df['text'].apply(lambda x: sift_tweet(str(x).lower(), stopwords))\n",
    "combined_cleaned_tweets = ' '.join(fetched_tweets_df['cleaned_tweet'])\n",
    "\n",
    "            \n",
    "\n",
    "# Get sentiment values on tweets using VADER sentiment analyzer\n",
    "sia = get_sentiment(combined_tweets)\n",
    "compound = sia['compound']                    # Score representing sum(lexicon ratings)\n",
    "pos = sia['pos']\n",
    "neg = sia['neg']\n",
    "neu = sia['neu']\n",
    "\n",
    "live_coin_data.loc[live_coin_data.index[0],'compound'] = compound  \n",
    "live_coin_data.loc[live_coin_data.index[0],'pos'] = pos          \n",
    "live_coin_data.loc[live_coin_data.index[0],'neg'] = neg            \n",
    "live_coin_data.loc[live_coin_data.index[0],'neu'] = neu \n",
    "live_coin_data.loc[live_coin_data.index[0],'polarity'] = TextBlob(combined_cleaned_tweets).sentiment[0]            \n",
    "live_coin_data.loc[live_coin_data.index[0],'subjectivity'] = TextBlob(combined_cleaned_tweets).sentiment[1]\n",
    "\n",
    "live_coin_data\n",
    "\n",
    "# make the prediction\n",
    "model.predict(live_coin_data)\n",
    "prob = model.predict_proba(live_coin_data)\n",
    "\n",
    "print(prob[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! The above cell outputs the prediction from the model. \n",
    "\n",
    "* First # signifies probability of a decrease in price\n",
    "* Second # signifiese probability of an increase in price"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4c0bf06c6142ddc920bc4833060833a5c39c864bf9bfacfcb217d05e37f17a2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
