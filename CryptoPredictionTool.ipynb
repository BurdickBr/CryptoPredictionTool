{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statements\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd                 # Pandas dataframe library\n",
    "import pandas_datareader as pdr     # Pandas datareader that allows me to lookup & store live crypto prices from yahoo finance.\n",
    "import numpy as np                  # Numpy\n",
    "from alpha_vantage.timeseries import TimeSeries     # Library used for pulling live price data from alphavantage api\n",
    "\n",
    "import secrets              # This import is referring to the secrets.py file supplied with the submission of the project.\n",
    "                            # Secrets contains the API keys that are used for AlphaVantage and the Twitter API.\n",
    "\n",
    "from datetime import datetime, timedelta, timezone             # Datetime library.\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=ResourceWarning)    # Suppresses warnings to limit size of output cells.\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import glob                         # For changing/finding proper directory\n",
    "import os                           # For changing/finding proper directory (when opening files)\n",
    "import requests                     # For sending HTTP requests in order to hit necessary API endpoints.\n",
    "import twint                        # Twitter web scraping tool with more features than the regular twitter API\n",
    "import nest_asyncio                 # Import required for twint usage, allows for the use of asynchronous computing\n",
    "nest_asyncio.apply()                \n",
    "\n",
    "import re                           # Regex for string cleaning (used for Textblob Sentiment Analysis)\n",
    "from textblob import TextBlob       # Textblob used for sentiment analysis of cleaned data.\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer    # Sentiment analysis tool that works great on determining social media sentiment.\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report       \n",
    "from sklearn.model_selection import train_test_split                    # Used for splitting data\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis    # Used for implementing LDA\n",
    "\n",
    "os.chdir(r'C:\\Users\\WaKaBurd\\Documents\\GitHub\\CryptoPredictionTool\\archive')    # Directory where the stopwords.txt file is located. \n",
    "stopwords_file = open(\"stopwords.txt\", \"r+\")                                    # This file is used for sifting the tweets fetched before feeding them into Textblob for Sentiment Analysis\n",
    "stopwords = list(stopwords_file.read().split('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in crypto price dataset\n",
    "---\n",
    "Section below reads csv files into pandas dataframes for interacting with. Also compiles list of coin names for twitter searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'c:\\Users\\WaKaBurd\\Documents\\GitHub\\CryptoPredictionTool\\prices\\HourlyPrices'\n",
    "extension = 'csv'\n",
    "os.chdir(path)\n",
    "hourly_csv_files = glob.glob('*.{}'.format(extension))\n",
    "\n",
    "# Compile list of all coin names for searching on twitter later\n",
    "hourly_coins = []\n",
    "\n",
    "for coin in hourly_csv_files:\n",
    "    vals = coin.split(\"_\")\n",
    "    coin_name = vals[0]\n",
    "    hourly_coins.append(coin_name)\n",
    "\n",
    "# compile list of pandas dataframes for use later.\n",
    "hourly_coin_data = []\n",
    "\n",
    "for file in hourly_csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    hourly_coin_data.append(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just to give you an idea of what this looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_coin_data[4].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Twitter for data on all coins supplied by dataset\n",
    "---\n",
    "Below section of code searches through Twint tweet database for any tweets associated \n",
    "with the each of the provided cryptocurrency acronyms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for iterating through coins list and storing findings in .csv files\n",
    "def search_coins(coins):\n",
    "    \n",
    "    for coin in coins:\n",
    "        path = r'c:\\Users\\WaKaBurd\\Documents\\GitHub\\CryptoPredictionTool\\search_results'\n",
    "        os.chdir(path)\n",
    "        os.chdir(coin)\n",
    "        \n",
    "        print('performing twitter search for coin:', coin)\n",
    "        \n",
    "        from_date = '2022-05-08'\n",
    "        to_date = '2022-05-10'\n",
    "        print(f'searching {from_date} to {to_date}')\n",
    "        \n",
    "        c = twint.Config()\n",
    "        c.Limit = 100\n",
    "        c.Lang = \"en\"\n",
    "        c.Pandas = True\n",
    "        c.Search = coin\n",
    "        c.Hide_output = True\n",
    "        c.Since = from_date\n",
    "        c.Until = to_date\n",
    "        c.Store_csv = True\n",
    "        c.Output = coin + '_' + from_date + '_' + to_date + '_search_result.csv'\n",
    "        twint.run.Search(c)\n",
    "search_coins(hourly_coins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below chunk is where the data pre-processing begins.\n",
    "---\n",
    "**Purpose:**\n",
    "* I need to modify the dataframe so that it contains both the price information, as well as all of the tweets so I can easily perform sentiment analysis on them using VADER Sentiment Analysis & Textblob.\n",
    "\n",
    "The two functions below are used for the following: \n",
    "* The *sift_tweet* function the tweets for textblob, as that tool will be providing us with the **subjectivity** and **polarity** of the tweets we've scraped, but requires there to be no unnecessary characters (emojis, hashtags, links, etc.). \n",
    "* The *get_sentiment* function will run the base tweet through the Vader Sentiment Intensity Analyzer to derive the **compound, positive, negative**, and **neutral** values for the text provided. \n",
    "* *NOTE:* VADER is deisgned to be able to accept and analyze text taken from an online space, meaning it knows how to interpret emojis, hashtags and slang to an extent. As a result, the tweets fed in here are **not** sifted like the above tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to create function for cleaning the tweets so we can derive the subjectivity and polarity using textblob.\n",
    "def sift_tweet(tweet, stop_words):\n",
    "    cleaned_tweet = tweet\n",
    "    cleaned_tweet = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",tweet) # regex to remove all @userame, emojis, and links from tweets.\n",
    "    for word in cleaned_tweet:\n",
    "        if word in stop_words: cleaned_tweet.replace(word, '')\n",
    "    return cleaned_tweet\n",
    "\n",
    "# Function for allowing me to generate the sentiment intensity of the text passed in.\n",
    "def get_sentiment(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below there are a few important things that happen:\n",
    "\n",
    "* I iterate through the list of coins we have hourly price data for and comprise dataframes for each coin with its respective tweets from the search results.\n",
    "* From there the I go coin by coin, creating the desired shape of the dataframe, then filling in all appropriate values. These include:\n",
    "  * breaking the tweets up by hour and sorting them with their appropriate time window\n",
    "  * cleaning the tweets using the sift tweet function\n",
    "  * Running the sentiment analysis on the tweets and storing those values in their corresponding cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hourly_coin_data[4]['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\WaKaBurd\\Documents\\GitHub\\CryptoPredictionTool\\search_results')\n",
    "tweet_dfs = []\n",
    "grouped_tweets = []\n",
    "\n",
    "# Read Tweets into a DF from the CSVs\n",
    "for coin in hourly_coins:\n",
    "    \n",
    "    os.chdir(r'C:\\Users\\WaKaBurd\\Documents\\GitHub\\CryptoPredictionTool\\search_results')\n",
    "    os.chdir(coin)\n",
    "    csv_names = glob.glob('*.{}'.format(extension))\n",
    "    coin_pds = []\n",
    "    for file in csv_names:\n",
    "        tweet_pd = pd.read_csv(file)\n",
    "        tweet_pd.sort_values(by='created_at')\n",
    "        coin_pds.append(tweet_pd)\n",
    "    tweet_dfs.append(coin_pds)\n",
    "\n",
    "\n",
    "# This is just so i can the data i need to train a model for aave and avax. I'll do all 5 when i want to showcase something but for now i only need these 2.\n",
    "for i in range(len(tweet_dfs)):\n",
    "    print('lookin at coin number:', i)\n",
    "    hourly_coin_data[i]['date'] = pd.to_datetime(hourly_coin_data[i]['date'])\n",
    "    hourly_coin_data[i]['joined_tweets'] = \"\"\n",
    "    hourly_coin_data[i]['compound'] = 0.0\n",
    "    hourly_coin_data[i]['positive'] = 0.0\n",
    "    hourly_coin_data[i]['negative'] = 0.0\n",
    "    hourly_coin_data[i]['neutral'] = 0.0\n",
    "\n",
    "    #print(hourly_coin_data[i])\n",
    "    for j in range(len(tweet_dfs[i])):\n",
    "        tweet_dfs[i][j]['created_at'] = tweet_dfs[i][j]['created_at'].str.replace(\" Pacific Daylight Time\",\"\").str.strip()\n",
    "        tweet_dfs[i][j]['created_at'] = pd.to_datetime(tweet_dfs[i][j]['created_at'])\n",
    "\n",
    "        for day in range(1,31):\n",
    "            #print('checking day:', day)\n",
    "            for hour in range(24):\n",
    "                tweet_time_mask = (tweet_dfs[i][j]['created_at'].dt.hour >= hour) & (tweet_dfs[i][j]['created_at'].dt.hour < hour + 1) & \\\n",
    "                            (tweet_dfs[i][j]['created_at'].dt.day >= day ) & (tweet_dfs[i][j]['created_at'].dt.day < day + 1)\n",
    "                price_time_mask = (hourly_coin_data[i]['date'].dt.hour >= hour) & (hourly_coin_data[i]['date'].dt.hour < hour + 1) & \\\n",
    "                            (hourly_coin_data[i]['date'].dt.day >= day ) & (hourly_coin_data[i]['date'].dt.day < day + 1)\n",
    "\n",
    "                hour_view = tweet_dfs[i][j][tweet_time_mask]\n",
    "                if hour_view.empty:\n",
    "                    continue\n",
    "                \n",
    "                hour_view['cleaned_tweet'] = hour_view['tweet'].apply(lambda x: sift_tweet(str(x).lower(), stopwords))\n",
    "\n",
    "                joined_tweets = ' '.join(hour_view['tweet'])\n",
    "                joined_clean_tweets = ' '.join(hour_view['cleaned_tweet'])\n",
    "\n",
    "                SIA = get_sentiment(joined_tweets)\n",
    "                compound = SIA['compound']                    # Score representing sum(lexicon ratings)\n",
    "                pos = SIA['pos']\n",
    "                neg = SIA['neg']\n",
    "                neu = SIA['neu']\n",
    "\n",
    "                index = hourly_coin_data[i][price_time_mask].index\n",
    "                for ind in index:\n",
    "                    hourly_coin_data[i].at[ind,'joined_tweets'] = joined_tweets\n",
    "                    hourly_coin_data[i].at[ind,'polarity'] = TextBlob(joined_clean_tweets).sentiment[0]                # Analyze and store Polarity value in Coin Dataframe using Textblob\n",
    "                    hourly_coin_data[i].at[ind,'subjectivity'] = TextBlob(joined_clean_tweets).sentiment[1]            # Analyze and store Subjectivity value in Coin Dataframe using Textblob\n",
    "                    hourly_coin_data[i].at[ind,'compound'] = compound\n",
    "                    hourly_coin_data[i].at[ind,'positive'] = pos\n",
    "                    hourly_coin_data[i].at[ind,'negative'] = neg\n",
    "                    hourly_coin_data[i].at[ind,'neutral'] = neu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the rows that don't contain a polarity score. The only reason they wouldn't have this would be because they didn't have any tweets stored in their row for that hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(hourly_coin_data)):\n",
    "    hourly_coin_data[i] = hourly_coin_data[i][hourly_coin_data[i]['polarity'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I have to iterate through remaining rows and append a price change label. This label signifies whether or not in that hour the price of the coin went up or down. This is what the model is going to be responsible for predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(hourly_coin_data)):\n",
    "    hourly_coin_data[i].reset_index()\n",
    "    hourly_coin_data[i]['price_change'] = np.nan\n",
    "    for index, row in hourly_coin_data[i].iterrows():\n",
    "        if row.open > row.close:\n",
    "            hourly_coin_data[i].at[index, 'price_change'] = 0\n",
    "        else:\n",
    "            hourly_coin_data[i].at[index, 'price_change'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_coin_data[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_coin_data[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Implementation: LDA With Sentiment Analysis\n",
    "---\n",
    "Why I'm using LDA:\n",
    "* I figured instead of using an LSTMRNN (which had very poor performance) I could try twisting the problem and using a classification model instead.\n",
    "* The goal now is to form the data into a format which can allow the model to make a prediction based on a label describing whether or not it believes the price will increase/decrease over the next hour.\n",
    "* Uses Naive Bayes to determine what it should be classified as (increasing/decreasing), then we can display that probability in our front-end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT:\n",
    "---\n",
    "* If you're running each cell in the jupyter notebook, you only need to run the below code cell. \n",
    "\n",
    "* If you're going to try to use the exported model_df_#.csv files that are saved in the hourly_coin_data directory, you need to run the 2nd cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the columns we actually want to keep for the purposes of training & using the model.\n",
    "model_cols = ['open', 'high', 'low', 'Volume USD', 'compound', 'positive', 'negative', 'neutral', 'polarity', 'subjectivity', 'price_change']\n",
    "os.chdir(r'C:\\Users\\WaKaBurd\\Documents\\GitHub\\CryptoPredictionTool\\hourly_coin_data')\n",
    "\n",
    "#for i in range(1):\n",
    "for i in range(3):\n",
    "    model_df = hourly_coin_data[i][model_cols]\n",
    "    model_df.to_csv(f'model_df_{i}.csv')\n",
    "\n",
    "    # Feature Dataset\n",
    "    x = model_df\n",
    "    # Target Dataset\n",
    "    y = np.array(model_df['price_change'])\n",
    "    x.drop(['price_change'], axis=1, inplace=True)\n",
    "    np.asarray(x)\n",
    "    \n",
    "    # split into test & train\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    # Create LDA model\n",
    "    model = LinearDiscriminantAnalysis().fit(x_train, y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE:* As stated above the below codeblock is mainly for testing purposes. It allows me to read in the previously pre-processed and formatted data for ease of use and reduces the wait time required for sentiment analysis TREMENDOUSLY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the columns we actually want to keep for the purposes of training & using the model.\n",
    "model_cols = ['open', 'high', 'low', 'Volume USD', 'compound', 'positive', 'negative', 'neutral', 'polarity', 'subjectivity', 'price_change']\n",
    "os.chdir(r'C:\\Users\\WaKaBurd\\Documents\\GitHub\\CryptoPredictionTool\\hourly_coin_data')\n",
    "\n",
    "model_df = pd.read_csv('model_df_2.csv')\n",
    "model_df = model_df.iloc[: , 1:]                # Drops first column in the dataframe as we don't want/need it.\n",
    "\n",
    "# Feature Dataset\n",
    "x = model_df\n",
    "# Target Dataset\n",
    "y = np.array(model_df['price_change'])\n",
    "x.drop(['price_change'], axis=1, inplace=True)\n",
    "np.asarray(x)\n",
    "\n",
    "\n",
    "# split into test & train\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# Create LDA model\n",
    "model = LinearDiscriminantAnalysis().fit(x_train, y_train)\n",
    "predictions = model.predict(x_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull data from the last hour to make prediction\n",
    "---\n",
    "## Data I need:\n",
    "* Price by hour data for the currencies the model was trained on\n",
    "* Tweets for the last hour about that currency\n",
    "\n",
    "\n",
    "Below functions are responsible for:\n",
    "* Sending URL Request/hitting endpoint for alphavantage (pulls live crypto price data)\n",
    "* Hitting Twitter API endpoint for pulling tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request(url, headers, params, next_token=None):\n",
    "    params['next_token'] = next_token\n",
    "    response = requests.request('GET', url, headers=headers, params=params)\n",
    "    print('Endpoint response code:' + str(response.status_code))\n",
    "    if (response.status_code != 200):\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "def pull_live_tweets(coin):\n",
    "\n",
    "    # Pull tweets from the last hour\n",
    "    path = r'c:\\Users\\WaKaBurd\\Documents\\GitHub\\CryptoPredictionTool\\predicted_trends'\n",
    "    os.chdir(path)\n",
    "\n",
    "    print('performing twitter search for coin:', coin)\n",
    "\n",
    "    # 1 hour ago\n",
    "    from_date = datetime.now(timezone.utc) - timedelta(hours = 1)\n",
    "    to_date = datetime.now(timezone.utc) + timedelta(seconds=-30)\n",
    "    \n",
    "    iso_from_date = from_date.isoformat()\n",
    "    iso_to_date = to_date.isoformat()\n",
    "\n",
    "    from_date = from_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    to_date = to_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    print(f'searching {from_date} to {to_date}')\n",
    "    \n",
    "    bearer_token = secrets.bearer_token\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer {}\".format(bearer_token)\n",
    "    }\n",
    "\n",
    "    url = 'https://api.twitter.com/2/tweets/search/recent'\n",
    "\n",
    "    params = {\n",
    "        'query': coin,\n",
    "        'start_time': iso_from_date,\n",
    "        'end_time': iso_to_date,\n",
    "        'max_results': 100,\n",
    "        'next_token':{}\n",
    "    }\n",
    "\n",
    "    json_response = send_request(url, headers, params)\n",
    "    return json_response\n",
    "\n",
    "# Pull tweets on topic from last 30 minutes\n",
    "fetched_tweets = pull_live_tweets('AVAX lang:en')\n",
    "fetched_tweets_df = pd.DataFrame(fetched_tweets['data'])\n",
    "fetched_tweets_df.to_csv('recently_fetched_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses AlphaVantage API with their CRYPTO_INTRADAY endpoint.\n",
    "\n",
    "av_api_key = secrets.av_api_key\n",
    "path = r'c:\\Users\\WaKaBurd\\Documents\\GitHub\\CryptoPredictionTool\\prices\\LivePrices'\n",
    "os.chdir(path)\n",
    "\n",
    "def get_prices(coin):\n",
    "    url = f'https://www.alphavantage.co/query?function=CRYPTO_INTRADAY&symbol={coin}&market=USD&interval=1min&apikey={av_api_key}&datatype=csv'\n",
    "    req = requests.get(url)\n",
    "    data = req.content\n",
    "    csv_file = open(f'{coin}_prices.csv','wb')\n",
    "    csv_file.write(data)\n",
    "    csv_file.close()\n",
    "    return\n",
    "\n",
    "get_prices('AVAX')  # Get the prices from the specified coin\n",
    "\n",
    "# format that data into a dataframe\n",
    "live_prices = pd.read_csv('AVAX_prices.csv')    # read in live prices csv\n",
    "kept_prices = live_prices.head(60)              # keep only the last 60 minutes.\n",
    "high = kept_prices['high'].max(axis=0)       # Find the max value in the last 60 minutes\n",
    "low = kept_prices['low'].min(axis=0)        # find the lowesst value in the last 60 minutes\n",
    "open_price = kept_prices['open'].values[59]                 # Price from 60 minutes ago. (opening price of the last hour) \n",
    "volume = kept_prices['volume'].sum(axis=0)      # summate the total volume traded from the last hour\n",
    "\n",
    "live_coin_data = pd.DataFrame([[open_price, high, low, volume]], columns =['open', 'high', 'low', 'volume'])\n",
    "\n",
    "# Run textblob on tweets for polarity & subjectivity\n",
    "combined_tweets = ' '.join(fetched_tweets_df['text'])\n",
    "\n",
    "# Clean tweet so we can use textblob on it.\n",
    "fetched_tweets_df['cleaned_tweet'] = fetched_tweets_df['text'].apply(lambda x: sift_tweet(str(x).lower(), stopwords))\n",
    "combined_cleaned_tweets = ' '.join(fetched_tweets_df['cleaned_tweet'])\n",
    "\n",
    "# Get sentiment values on tweets using VADER sentiment analyzer\n",
    "sia = get_sentiment(combined_tweets)\n",
    "compound = sia['compound']                    # Score representing sum(lexicon ratings)\n",
    "pos = sia['pos']\n",
    "neg = sia['neg']\n",
    "neu = sia['neu']\n",
    "\n",
    "live_coin_data.loc[live_coin_data.index[0],'compound'] = compound\n",
    "live_coin_data.loc[live_coin_data.index[0],'pos'] = pos\n",
    "live_coin_data.loc[live_coin_data.index[0],'neg'] = neg\n",
    "live_coin_data.loc[live_coin_data.index[0],'neu'] = neu\n",
    "live_coin_data.loc[live_coin_data.index[0],'polarity'] = TextBlob(combined_cleaned_tweets).sentiment[0]            \n",
    "live_coin_data.loc[live_coin_data.index[0],'subjectivity'] = TextBlob(combined_cleaned_tweets).sentiment[1]\n",
    "\n",
    "# make the prediction\n",
    "model.predict(live_coin_data)\n",
    "prob = model.predict_proba(live_coin_data)\n",
    "\n",
    "print(prob[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prob[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! The above cell outputs the prediction from the model. \n",
    "\n",
    "* First # signifies probability of a decrease in price\n",
    "* Second # signifiese probability of an increase in price"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4c0bf06c6142ddc920bc4833060833a5c39c864bf9bfacfcb217d05e37f17a2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
